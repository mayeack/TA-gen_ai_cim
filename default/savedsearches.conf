#
# savedsearches.conf - Governance alerts and scheduled searches
# TA-gen_ai_cim
#
# Production-ready alerts for AI governance, safety, and compliance monitoring
#

###############################################################################
# SAFETY VIOLATION ALERTS
###############################################################################

[GenAI - Safety Violation Alert]
description = Triggers when AI models produce responses that violate safety policies
search = index=gen_ai_log \
    gen_ai.safety.violated="true" \
| eval severity=case(\
    like('gen_ai.safety.categories', "%EMERGENCY%"), "CRITICAL",\
    like('gen_ai.safety.categories', "%HIGH%"), "HIGH",\
    like('gen_ai.safety.categories', "%MEDIUM%"), "MEDIUM",\
    1=1, "LOW"\
) \
| stats count as violation_count, \
    values(gen_ai.safety.categories) as categories, \
    values(gen_ai.request.model) as models, \
    values(gen_ai.app.name) as apps, \
    dc(gen_ai.session.id) as unique_sessions \
    by severity, gen_ai.deployment.id \
| where violation_count > 0
dispatch.earliest_time = -15m
dispatch.latest_time = now
cron_schedule = */15 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
action.email = 1
action.email.to = ai-safety-team@example.com
action.email.subject = GenAI Safety Violation Detected - $result.severity$
action.email.message.alert = Safety violations detected in GenAI operations. Review immediately.\n\nSeverity: $result.severity$\nViolation Count: $result.violation_count$\nDeployment: $result.gen_ai.deployment.id$\nCategories: $result.categories$

[GenAI - Critical Safety Alert - EMERGENCY]
description = Immediate alert for EMERGENCY-level safety violations (e.g., medical emergencies, harm)
search = index=gen_ai_log \
    gen_ai.safety.violated="true" \
    (gen_ai.safety.categories="*EMERGENCY*" OR gen_ai.safety.categories="*CRITICAL*") \
| table _time, gen_ai.deployment.id, gen_ai.app.name, gen_ai.request.model, gen_ai.safety.categories, gen_ai.session.id, gen_ai.request.id, gen_ai.guardrail.ids
dispatch.earliest_time = -5m
dispatch.latest_time = now
cron_schedule = */5 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 0
alert.severity = 5
alert_type = always
action.email = 1
action.email.to = ai-safety-oncall@example.com
action.email.subject = URGENT: GenAI EMERGENCY Safety Violation
action.email.priority = 1

###############################################################################
# PII DETECTION ALERTS
###############################################################################

[GenAI - PII Detection Alert]
description = Detects PII in AI model inputs or outputs
search = index=gen_ai_log \
    gen_ai.pii.detected="true" \
| stats count as pii_event_count, \
    values(gen_ai.pii.types) as pii_types, \
    values(gen_ai.request.model) as models, \
    values(gen_ai.app.name) as apps, \
    dc(gen_ai.session.id) as affected_sessions \
    by gen_ai.deployment.id \
| where pii_event_count > 0
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
action.email = 1
action.email.to = privacy-team@example.com, ai-governance@example.com
action.email.subject = GenAI PII Detection Alert - $result.gen_ai.deployment.id$

[GenAI - PII High Volume Alert]
description = Triggers when PII detection rate exceeds threshold
search = index=gen_ai_log \
    (gen_ai.pii.detected="true" OR gen_ai.pii.detected="false") \
| stats count as total_events, \
    sum(eval(if('gen_ai.pii.detected'="true", 1, 0))) as pii_events \
    by gen_ai.deployment.id, gen_ai.app.name \
| eval pii_rate=round((pii_events/total_events)*100, 2) \
| where pii_rate > 5 \
| eval alert_message="PII detection rate of ".pii_rate."% exceeds 5% threshold"
dispatch.earliest_time = -4h
dispatch.latest_time = now
cron_schedule = 0 */4 * * *
enableSched = 1
alert.track = 1
alert.severity = 4
alert_type = always

[GenAI - MLTK PII Risk Score Alert]
description = Alerts on high PII risk scores from MLTK model
search = index=gen_ai_log \
    gen_ai.pii.risk_score>0.7 \
| stats count as high_risk_count, \
    avg(gen_ai.pii.risk_score) as avg_risk_score, \
    max(gen_ai.pii.risk_score) as max_risk_score \
    by gen_ai.deployment.id, gen_ai.app.name \
| where high_risk_count > 0
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 4

###############################################################################
# PROMPT INJECTION DETECTION ALERTS
###############################################################################

[GenAI - Prompt Injection Alert]
description = Detects potential prompt injection attacks using MLTK model
search = index=gen_ai_log \
    gen_ai.prompt_injection.ml_detected="true" \
| stats count as injection_attempts, \
    avg(gen_ai.prompt_injection.risk_score) as avg_risk, \
    max(gen_ai.prompt_injection.risk_score) as max_risk, \
    values(gen_ai.prompt_injection.technique) as techniques, \
    dc(client.address) as unique_sources, \
    dc(gen_ai.session.id) as unique_sessions \
    by gen_ai.deployment.id, gen_ai.app.name \
| where injection_attempts > 0
dispatch.earliest_time = -30m
dispatch.latest_time = now
cron_schedule = */30 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
alert_type = always
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = GenAI Prompt Injection Detected - $result.gen_ai.deployment.id$

[GenAI - Prompt Injection by Source IP]
description = Identifies sources with repeated prompt injection attempts
search = index=gen_ai_log \
    gen_ai.prompt_injection.risk_score>0.6 \
| stats count as attempts, \
    avg(gen_ai.prompt_injection.risk_score) as avg_risk, \
    values(gen_ai.prompt_injection.technique) as techniques, \
    values(gen_ai.app.name) as apps \
    by client.address \
| where attempts >= 3 \
| sort -attempts
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 3

###############################################################################
# MODEL DRIFT AND QUALITY ALERTS
###############################################################################

[GenAI - Model Drift Critical Alert]
description = Alerts when model drift status reaches critical level
search = index=gen_ai_log \
    gen_ai.drift.status="critical" \
| stats count as critical_events, \
    values(gen_ai.drift.metric.name) as drift_metrics, \
    avg(gen_ai.drift.metric.value) as avg_drift_value \
    by gen_ai.deployment.id, gen_ai.request.model \
| where critical_events > 0
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
alert_type = always
action.email = 1
action.email.to = ml-ops@example.com, ai-governance@example.com
action.email.subject = GenAI Model Drift Critical - $result.gen_ai.request.model$

###############################################################################
# LATENCY AND PERFORMANCE ALERTS
###############################################################################

[GenAI - Latency Outlier Alert]
description = Detects when response latency exceeds 2x the average (P95 threshold)
search = index=gen_ai_log \
    gen_ai.client.operation.duration>0 \
| stats avg(gen_ai.client.operation.duration) as avg_duration, \
    perc95(gen_ai.client.operation.duration) as p95_duration, \
    max(gen_ai.client.operation.duration) as max_duration, \
    count as request_count \
    by gen_ai.request.model, gen_ai.deployment.id \
| eval threshold=avg_duration*2 \
| where p95_duration > threshold \
| eval alert_msg="P95 latency (".round(p95_duration, 2)."s) exceeds 2x avg (".round(avg_duration, 2)."s)"
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 3
alert_type = always

[GenAI - Slow Response Alert]
description = Alerts when responses take longer than 10 seconds
search = index=gen_ai_log \
    gen_ai.client.operation.duration>10 \
| stats count as slow_requests, \
    avg(gen_ai.client.operation.duration) as avg_slow_duration, \
    max(gen_ai.client.operation.duration) as max_duration \
    by gen_ai.request.model, gen_ai.deployment.id \
| where slow_requests > 5
dispatch.earliest_time = -15m
dispatch.latest_time = now
cron_schedule = */15 * * * *
enableSched = 1
alert.track = 1
alert.severity = 3

###############################################################################
# COST ANOMALY ALERTS
###############################################################################

[GenAI - Cost Spike Alert]
description = Detects unusual spikes in AI model costs
search = index=gen_ai_log \
    gen_ai.cost.total>0 \
| bucket _time span=1h \
| stats sum(gen_ai.cost.total) as hourly_cost \
    by _time, gen_ai.deployment.id, gen_ai.request.model \
| streamstats window=24 avg(hourly_cost) as avg_24h_cost by gen_ai.deployment.id, gen_ai.request.model \
| eval cost_ratio=hourly_cost/avg_24h_cost \
| where cost_ratio > 2 \
| eval alert_msg="Hourly cost ($".round(hourly_cost, 2).") is ".round(cost_ratio, 1)."x the 24h average"
dispatch.earliest_time = -2h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 3
alert_type = always

[GenAI - High Token Usage Alert]
description = Alerts when token usage exceeds expected thresholds
search = index=gen_ai_log \
    gen_ai.usage.total_tokens>0 \
| stats sum(gen_ai.usage.total_tokens) as total_tokens, \
    avg(gen_ai.usage.total_tokens) as avg_tokens, \
    count as request_count \
    by gen_ai.deployment.id, gen_ai.request.model \
| where avg_tokens > 5000 OR total_tokens > 1000000
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 3

###############################################################################
# POLICY AND COMPLIANCE ALERTS
###############################################################################

[GenAI - Policy Block Alert]
description = Tracks when requests are blocked by policy enforcement
search = index=gen_ai_log \
    gen_ai.policy.blocked="true" \
| stats count as blocked_count, \
    dc(gen_ai.session.id) as unique_sessions, \
    dc(client.address) as unique_sources \
    by gen_ai.deployment.id, gen_ai.app.name \
| where blocked_count > 0
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 3

[GenAI - Guardrail Trigger Summary]
description = Daily summary of all guardrail activations
search = index=gen_ai_log \
    gen_ai.guardrail.triggered="true" \
| stats count as trigger_count, \
    dc(gen_ai.session.id) as affected_sessions \
    by gen_ai.guardrail.ids, gen_ai.deployment.id, gen_ai.app.name \
| sort -trigger_count
dispatch.earliest_time = -24h
dispatch.latest_time = now
cron_schedule = 0 8 * * *
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ai-governance@example.com
action.email.subject = Daily GenAI Guardrail Summary

###############################################################################
# ERROR AND FAILURE ALERTS
###############################################################################

[GenAI - Error Rate Alert]
description = Alerts when error rate exceeds 5% of total requests
search = index=gen_ai_log \
    (gen_ai.status=* OR error.type=*) \
| stats count as total_requests, \
    sum(eval(if(gen_ai.status!="success" OR isnotnull('error.type'), 1, 0))) as error_count \
    by gen_ai.deployment.id, gen_ai.request.model \
| eval error_rate=round((error_count/total_requests)*100, 2) \
| where error_rate > 5 \
| eval alert_msg="Error rate of ".error_rate."% exceeds 5% threshold"
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 4

[GenAI - Model Failure Alert]
description = Immediate alert for model failures or errors
search = index=gen_ai_log \
    (gen_ai.status="failure" OR gen_ai.status="error" OR isnotnull(error.type)) \
| stats count as failure_count, \
    values(error.type) as error_types, \
    values(error.message) as error_messages \
    by gen_ai.deployment.id, gen_ai.request.model \
| where failure_count > 0
dispatch.earliest_time = -15m
dispatch.latest_time = now
cron_schedule = */15 * * * *
enableSched = 1
alert.track = 1
alert.severity = 4

###############################################################################
# TF-IDF ANOMALY DETECTION - MODEL TRAINING
# 
# IMPORTANT: Run these searches IN ORDER (Step 1, then Step 2, then Step 3)
# Each step must complete before running the next step.
###############################################################################

[GenAI - TFIDF Train Prompt Step 1 - PCA Model]
description = Step 1 of 2: Train PCA model for prompts using HashingVectorizer. Run this FIRST, wait for completion, then run Step 2. Uses curated training data from lookup.
search = | inputlookup tfidf_training_data_v3 \
| rename prompt AS input_text \
| where isnotnull(input_text) AND len(input_text) > 10 \
| eval input_text_clean=lower(input_text) \
| eval input_text_clean=replace(input_text_clean, "[^a-z0-9\s]", " ") \
| eval input_text_clean=replace(input_text_clean, "\s+", " ") \
| eval input_text_clean=trim(input_text_clean) \
| where len(input_text_clean) > 20 \
| head 20000 \
| fit HashingVectorizer input_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| fit PCA "input_text_clean_hashed_*" k=50 into app:tfidf_prompt_pca
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - TFIDF Train Prompt Step 2 - Anomaly Model]
description = Step 2 of 2: Train OneClassSVM anomaly model for prompts. Run AFTER Step 1 completes. Uses curated training data from lookup.
search = | inputlookup tfidf_training_data_v3 \
| rename prompt AS input_text \
| where isnotnull(input_text) AND len(input_text) > 10 \
| eval input_text_clean=lower(input_text) \
| eval input_text_clean=replace(input_text_clean, "[^a-z0-9\s]", " ") \
| eval input_text_clean=replace(input_text_clean, "\s+", " ") \
| eval input_text_clean=trim(input_text_clean) \
| where len(input_text_clean) > 20 \
| head 20000 \
| fit HashingVectorizer input_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| apply app:tfidf_prompt_pca \
| fit OneClassSVM "PC_*" kernel=rbf nu=0.25 into app:prompt_anomaly_model
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - TFIDF Train Response Step 1 - PCA Model]
description = Step 1 of 2: Train PCA model for responses using HashingVectorizer. Run this FIRST, wait for completion, then run Step 2. Uses curated training data from lookup.
search = | inputlookup tfidf_training_data_v3 \
| rename response AS output_text \
| where isnotnull(output_text) AND len(output_text) > 20 \
| eval output_text_clean=lower(output_text) \
| eval output_text_clean=replace(output_text_clean, "[^a-z0-9\s]", " ") \
| eval output_text_clean=replace(output_text_clean, "\s+", " ") \
| eval output_text_clean=trim(output_text_clean) \
| where len(output_text_clean) > 50 \
| head 20000 \
| fit HashingVectorizer output_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| fit PCA "output_text_clean_hashed_*" k=20 into app:tfidf_response_pca
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - TFIDF Train Response Step 2 - Anomaly Model]
description = Step 2 of 2: Train OneClassSVM anomaly model for responses. Run AFTER Step 1 completes. Uses curated training data from lookup.
search = | inputlookup tfidf_training_data_v3 \
| rename response AS output_text \
| where isnotnull(output_text) AND len(output_text) > 20 \
| eval output_text_clean=lower(output_text) \
| eval output_text_clean=replace(output_text_clean, "[^a-z0-9\s]", " ") \
| eval output_text_clean=replace(output_text_clean, "\s+", " ") \
| eval output_text_clean=trim(output_text_clean) \
| where len(output_text_clean) > 50 \
| head 20000 \
| fit HashingVectorizer output_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| apply app:tfidf_response_pca \
| fit OneClassSVM "PC_*" kernel=rbf nu=0.25 into app:response_anomaly_model
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

###############################################################################
# TF-IDF ANOMALY DETECTION - SCHEDULED SCORING
###############################################################################

[GenAI - TFIDF Scoring - Prompt Anomalies]
description = Apply hybrid anomaly detection (ML + pattern-based) to score prompts every minute. Uses macros for preprocessing, scoring, and output.
search = index=gen_ai_log sourcetype!="gen_ai:*:scoring" earliest=-1m@m latest=now \
| `genai_tfidf_preprocess_prompt` \
| dedup gen_ai.event.id \
| `genai_tfidf_score_prompt` \
| `genai_tfidf_output_prompt`
dispatch.earliest_time = -1m@m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
alert.track = 0

[GenAI - TFIDF Scoring - Response Anomalies]
description = Apply hybrid anomaly detection (ML + pattern-based) to score responses every minute. Uses macros for preprocessing, scoring, and output.
search = index=gen_ai_log sourcetype!="gen_ai:*:scoring" earliest=-1m@m latest=now \
| `genai_tfidf_preprocess_response` \
| dedup gen_ai.event.id \
| `genai_tfidf_score_response` \
| `genai_tfidf_output_response`
dispatch.earliest_time = -1m@m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
alert.track = 0

[GenAI - TFIDF Scoring - Combined Anomalies]
description = Combined hybrid anomaly scoring (ML + pattern-based) for both prompts and responses with risk level assessment. Uses macros for preprocessing, scoring, and output.
search = index=gen_ai_log sourcetype!="gen_ai:*:scoring" earliest=-1m@m latest=now \
| `genai_tfidf_preprocess_combined` \
| dedup gen_ai.event.id \
| `genai_tfidf_score_combined` \
| `genai_tfidf_combined_risk` \
| `genai_tfidf_output_combined`
dispatch.earliest_time = -1m@m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
alert.track = 0

###############################################################################
# TF-IDF ANOMALY DETECTION - ALERTS
###############################################################################

[GenAI - TFIDF Anomalous Prompt Alert]
description = Alerts when TF-IDF model detects anomalous prompts indicating potential misuse or attacks
search = index=gen_ai_log \
    gen_ai.prompt.is_anomaly="true" \
| stats count as anomalous_prompt_count, \
    dc(gen_ai.session.id) as unique_sessions, \
    dc(client.address) as unique_sources, \
    values(gen_ai.app.name) as apps, \
    avg(gen_ai.prompt.anomaly_score) as avg_anomaly_score \
    by gen_ai.deployment.id \
| where anomalous_prompt_count >= 3
dispatch.earliest_time = -15m
dispatch.latest_time = now
cron_schedule = */15 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
action.email = 1
action.email.to = ai-governance@example.com
action.email.subject = GenAI TF-IDF Anomalous Prompts Detected - $result.gen_ai.deployment.id$
action.email.message.alert = TF-IDF anomaly detection has identified unusual prompts.\n\nDeployment: $result.gen_ai.deployment.id$\nAnomalous Prompts: $result.anomalous_prompt_count$\nUnique Sessions: $result.unique_sessions$\nUnique Sources: $result.unique_sources$\nAvg Anomaly Score: $result.avg_anomaly_score$

[GenAI - TFIDF Anomalous Response Alert]
description = Alerts when TF-IDF model detects anomalous AI responses indicating potential quality issues or errors
search = index=gen_ai_log \
    gen_ai.response.is_anomaly="true" \
| stats count as anomalous_response_count, \
    dc(gen_ai.event.id) as unique_events, \
    values(gen_ai.request.model) as models, \
    values(gen_ai.app.name) as apps, \
    avg(gen_ai.response.anomaly_score) as avg_anomaly_score \
    by gen_ai.deployment.id \
| where anomalous_response_count >= 2
dispatch.earliest_time = -15m
dispatch.latest_time = now
cron_schedule = */15 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
action.email = 1
action.email.to = ai-governance@example.com
action.email.subject = GenAI TF-IDF Anomalous Responses Detected - $result.gen_ai.deployment.id$
action.email.message.alert = TF-IDF anomaly detection has identified unusual AI responses.\n\nDeployment: $result.gen_ai.deployment.id$\nAnomalous Responses: $result.anomalous_response_count$\nModels: $result.models$\nAvg Anomaly Score: $result.avg_anomaly_score$

[GenAI - TFIDF High Risk Combined Anomaly Alert]
description = URGENT - Both prompt and response anomalies detected in the same request (HIGH RISK)
search = index=gen_ai_log \
    gen_ai.tfidf.combined_anomaly="both" OR gen_ai.tfidf.risk_level="HIGH" \
| stats count as high_risk_count, \
    dc(gen_ai.session.id) as affected_sessions, \
    dc(client.address) as unique_sources, \
    values(gen_ai.app.name) as apps, \
    values(gen_ai.request.model) as models \
    by gen_ai.deployment.id \
| where high_risk_count >= 1
dispatch.earliest_time = -30m
dispatch.latest_time = now
cron_schedule = */30 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 0
alert.severity = 4
alert_type = always
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = URGENT: GenAI High-Risk TF-IDF Anomaly Detected
action.email.priority = 1
action.email.message.alert = CRITICAL: Both prompt AND response anomalies detected.\n\nDeployment: $result.gen_ai.deployment.id$\nHigh Risk Events: $result.high_risk_count$\nAffected Sessions: $result.affected_sessions$\nUnique Sources: $result.unique_sources$\n\nImmediate investigation recommended.

[GenAI - TFIDF Anomaly by Source IP Alert]
description = Identifies source IPs with repeated TF-IDF anomalies (potential attackers)
search = index=gen_ai_log \
    (gen_ai.prompt.is_anomaly="true" OR gen_ai.response.is_anomaly="true") \
| stats count as total_anomalies, \
    sum(eval(if('gen_ai.prompt.is_anomaly'="true", 1, 0))) as prompt_anomalies, \
    sum(eval(if('gen_ai.response.is_anomaly'="true", 1, 0))) as response_anomalies, \
    values(gen_ai.tfidf.combined_anomaly) as anomaly_types, \
    values(gen_ai.app.name) as apps, \
    dc(gen_ai.session.id) as unique_sessions \
    by client.address \
| where total_anomalies >= 5 \
| sort -total_anomalies
dispatch.earliest_time = -1h
dispatch.latest_time = now
cron_schedule = 0 * * * *
enableSched = 1
alert.track = 1
alert.severity = 4
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
action.email = 1
action.email.to = security-team@example.com
action.email.subject = GenAI TF-IDF Anomaly Source Identified - $result.client.address$

[GenAI - TFIDF Anomaly Rate Threshold Alert]
description = Alerts when overall TF-IDF anomaly rate exceeds acceptable threshold (potential model drift or attack)
search = index=gen_ai_log \
    (gen_ai.prompt.is_anomaly=* OR gen_ai.response.is_anomaly=*) \
| stats count as total_events, \
    sum(eval(if('gen_ai.prompt.is_anomaly'="true", 1, 0))) as anomalous_prompts, \
    sum(eval(if('gen_ai.response.is_anomaly'="true", 1, 0))) as anomalous_responses \
    by gen_ai.deployment.id, gen_ai.app.name \
| eval prompt_anomaly_rate=round((anomalous_prompts/total_events)*100, 2) \
| eval response_anomaly_rate=round((anomalous_responses/total_events)*100, 2) \
| where prompt_anomaly_rate > 10 OR response_anomaly_rate > 10 \
| eval alert_message=case( \
    prompt_anomaly_rate > 10 AND response_anomaly_rate > 10, "CRITICAL: Both prompt and response anomaly rates exceed threshold", \
    prompt_anomaly_rate > 10, "Prompt anomaly rate of ".prompt_anomaly_rate."% exceeds 10% threshold", \
    response_anomaly_rate > 10, "Response anomaly rate of ".response_anomaly_rate."% exceeds 10% threshold" \
)
dispatch.earliest_time = -4h
dispatch.latest_time = now
cron_schedule = 0 */4 * * *
enableSched = 1
alert.track = 1
alert.severity = 4
alert_type = always

###############################################################################
# TF-IDF ANOMALY DETECTION - REPORTS
###############################################################################

[GenAI - TFIDF Daily Anomaly Summary]
description = Daily summary of TF-IDF anomaly detection results
search = index=gen_ai_log \
    (gen_ai.prompt.is_anomaly=* OR gen_ai.response.is_anomaly=*) \
| stats count as total_events, \
    sum(eval(if('gen_ai.prompt.is_anomaly'="true", 1, 0))) as total_prompt_anomalies, \
    sum(eval(if('gen_ai.response.is_anomaly'="true", 1, 0))) as total_response_anomalies, \
    sum(eval(if('gen_ai.tfidf.combined_anomaly'="both", 1, 0))) as high_risk_events, \
    dc(gen_ai.session.id) as affected_sessions, \
    dc(client.address) as unique_sources \
    by gen_ai.deployment.id, gen_ai.app.name \
| eval prompt_anomaly_rate=round((total_prompt_anomalies/total_events)*100, 2) \
| eval response_anomaly_rate=round((total_response_anomalies/total_events)*100, 2) \
| sort -high_risk_events, -total_prompt_anomalies
dispatch.earliest_time = -24h
dispatch.latest_time = now
cron_schedule = 0 8 * * *
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ai-governance@example.com
action.email.subject = Daily GenAI TF-IDF Anomaly Detection Summary

[GenAI - TFIDF Model Performance Report]
description = Weekly report on TF-IDF model performance metrics
search = index=gen_ai_log \
    (gen_ai.prompt.anomaly_score=* OR gen_ai.response.anomaly_score=*) \
| stats count as total_scored, \
    avg(gen_ai.prompt.anomaly_score) as avg_prompt_score, \
    stdev(gen_ai.prompt.anomaly_score) as stdev_prompt_score, \
    avg(gen_ai.response.anomaly_score) as avg_response_score, \
    stdev(gen_ai.response.anomaly_score) as stdev_response_score, \
    sum(eval(if('gen_ai.prompt.is_anomaly'="true", 1, 0))) as prompt_anomalies, \
    sum(eval(if('gen_ai.response.is_anomaly'="true", 1, 0))) as response_anomalies \
| eval prompt_anomaly_rate=round((prompt_anomalies/total_scored)*100, 2) \
| eval response_anomaly_rate=round((response_anomalies/total_scored)*100, 2) \
| eval model_health=case( \
    prompt_anomaly_rate > 15 OR response_anomaly_rate > 15, "CRITICAL - Retrain recommended", \
    prompt_anomaly_rate > 10 OR response_anomaly_rate > 10, "WARNING - Monitor closely", \
    1=1, "HEALTHY" \
)
dispatch.earliest_time = -7d
dispatch.latest_time = now
cron_schedule = 0 9 * * 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ai-governance@example.com, ml-ops@example.com
action.email.subject = Weekly GenAI TF-IDF Model Performance Report

###############################################################################
# PII DETECTION ML MODEL - TRAINING
###############################################################################

[GenAI - PII Train Step 1 - Feature Engineering from Initial Dataset]
description = Step 1 of 3: Prepare feature-engineered training data from initial dataset. Run this FIRST before training.
search = | inputlookup llm_pii_mixed_responses_200k max=200000 \
| eval response_text=coalesce(response, "") \
| eval pii_label=coalesce(pii_label, 0) \
| where len(response_text) > 0 \
| eval output_length=len(response_text) \
| eval word_count=mvcount(split(response_text, " ")) \
| rex field=response_text "(?<ssn_match>\d{3}-\d{2}-\d{4})" \
| eval has_ssn=if(isnotnull(ssn_match), 1, 0) \
| rex field=response_text "(?<email_match>[a-zA-Z0-9][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})" \
| eval has_email=if(isnotnull(email_match), 1, 0) \
| rex field=response_text "(?<phone_match>(\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})|(\d{3}\.\d{3}\.\d{4}))" \
| eval has_phone=if(isnotnull(phone_match), 1, 0) \
| rex field=response_text "(?<dob_match>(?:date of birth|DOB|born):?\s*(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}))" \
| eval has_dob=if(isnotnull(dob_match), 1, 0) \
| rex field=response_text "(?<address_match>\d+\s+[A-Za-z]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Court|Ct|Place|Pl),?\s+[A-Z][a-z]+,?\s+[A-Z]{2}\s+\d{5})" \
| eval has_address=if(isnotnull(address_match), 1, 0) \
| rex field=response_text "(?<cc_match>\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4})" \
| eval has_credit_card=if(isnotnull(cc_match), 1, 0) \
| rex field=response_text "(?<name_match>(?:patient|for|Hi|Mr\.|Mrs\.|Ms\.|Dr\.)\s+([A-Z][a-z]+\s+[A-Z][a-z]+))" \
| eval has_name=if(isnotnull(name_match), 1, 0) \
| eval digit_count=len(replace(response_text, "[^\d]", "")) \
| eval digit_ratio=if(output_length>0, round(digit_count/output_length, 4), 0) \
| eval special_char_count=len(replace(response_text, "[A-Za-z0-9\s]", "")) \
| eval special_char_ratio=if(output_length>0, round(special_char_count/output_length, 4), 0) \
| eval uppercase_count=len(replace(response_text, "[^A-Z]", "")) \
| eval uppercase_ratio=if(output_length>0, round(uppercase_count/output_length, 4), 0) \
| table response_text pii_label output_length word_count digit_ratio special_char_ratio uppercase_ratio \
    has_ssn has_email has_phone has_dob has_address has_credit_card has_name \
| outputlookup pii_training_data_engineered \
| stats count as total_rows, \
    sum(pii_label) as pii_positive_examples, \
    sum(eval(if(pii_label=0,1,0))) as clean_examples, \
    sum(has_ssn) as ssn_detected, \
    sum(has_email) as emails_detected, \
    sum(has_phone) as phones_detected, \
    sum(has_dob) as dob_detected, \
    sum(has_address) as address_detected, \
    sum(has_credit_card) as credit_card_detected, \
    sum(has_name) as names_detected \
| eval status="SUCCESS: Feature engineering complete" \
| eval pii_percentage=round((pii_positive_examples/total_rows)*100, 2)."%" \
| table status total_rows pii_positive_examples clean_examples pii_percentage ssn_detected emails_detected phones_detected dob_detected address_detected credit_card_detected names_detected
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - PII Train Step 2 - Logistic Regression Model]
description = Step 2 of 3: Train Logistic Regression model for PII detection. Run AFTER Step 1 completes.
search = | inputlookup pii_training_data_engineered \
| fit LogisticRegression pii_label \
    from output_length word_count digit_ratio special_char_ratio uppercase_ratio \
    has_ssn has_email has_phone has_dob has_address has_credit_card has_name \
    probabilities=true \
    into app:pii_detection_model
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

# Random Forest model removed - use LogisticRegression (Step 2) instead

[GenAI - PII Train Step 3 - Validate Model Performance]
description = Step 3 of 3: Validate model performance metrics. Run AFTER Step 2 completes.
search = | inputlookup pii_training_data_engineered \
| sample 10000 \
| apply pii_detection_model \
| eval predicted_pii='predicted(pii_label)' \
| eval predicted_class=if(predicted_pii>0.5, 1, 0) \
| eval true_positive=if(pii_label=1 AND predicted_class=1, 1, 0) \
| eval true_negative=if(pii_label=0 AND predicted_class=0, 1, 0) \
| eval false_positive=if(pii_label=0 AND predicted_class=1, 1, 0) \
| eval false_negative=if(pii_label=1 AND predicted_class=0, 1, 0) \
| stats sum(true_positive) as TP, \
    sum(true_negative) as TN, \
    sum(false_positive) as FP, \
    sum(false_negative) as FN \
| eval Total=TP+TN+FP+FN \
| eval Accuracy=round((TP+TN)/Total, 4) \
| eval Precision=round(TP/(TP+FP), 4) \
| eval Recall=round(TP/(TP+FN), 4) \
| eval F1_Score=round(2*(Precision*Recall)/(Precision+Recall), 4) \
| eval Specificity=round(TN/(TN+FP), 4) \
| table Accuracy Precision Recall F1_Score Specificity TP TN FP FN Total
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

###############################################################################
# PII DETECTION ML MODEL - SCHEDULED SCORING
###############################################################################

[GenAI - PII Scoring - Response Analysis]
description = Apply PII detection model to score AI responses every minute
search = index=gen_ai_log NOT sourcetype="gen_ai:pii:scoring" earliest=-1m@m latest=now \
| eval response_text='gen_ai.output.messages' \
| where isnotnull(response_text) AND len(response_text) > 20 \
| dedup gen_ai.event.id \
| eval output_length=len(response_text) \
| eval word_count=mvcount(split(response_text, " ")) \
| rex field=response_text "(?<ssn_match>\d{3}-\d{2}-\d{4})" \
| eval has_ssn=if(isnotnull(ssn_match), 1, 0) \
| rex field=response_text "(?<email_match>[a-zA-Z0-9][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})" \
| eval has_email=if(isnotnull(email_match), 1, 0) \
| rex field=response_text "(?<phone_match>(\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})|(\d{3}\.\d{3}\.\d{4}))" \
| eval has_phone=if(isnotnull(phone_match), 1, 0) \
| rex field=response_text "(?<dob_match>(?:date of birth|DOB|born):?\s*(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}))" \
| eval has_dob=if(isnotnull(dob_match), 1, 0) \
| rex field=response_text "(?<address_match>\d+\s+[A-Za-z]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Court|Ct|Place|Pl),?\s+[A-Z][a-z]+,?\s+[A-Z]{2}\s+\d{5})" \
| eval has_address=if(isnotnull(address_match), 1, 0) \
| rex field=response_text "(?<cc_match>\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4})" \
| eval has_credit_card=if(isnotnull(cc_match), 1, 0) \
| rex field=response_text "(?<name_match>(?:patient|for|Hi|Mr\.|Mrs\.|Ms\.|Dr\.)\s+([A-Z][a-z]+\s+[A-Z][a-z]+))" \
| eval has_name=if(isnotnull(name_match), 1, 0) \
| eval digit_count=len(replace(response_text, "[^\d]", "")) \
| eval digit_ratio=if(output_length>0, round(digit_count/output_length, 4), 0) \
| eval special_char_count=len(replace(response_text, "[A-Za-z0-9\s]", "")) \
| eval special_char_ratio=if(output_length>0, round(special_char_count/output_length, 4), 0) \
| eval uppercase_count=len(replace(response_text, "[^A-Z]", "")) \
| eval uppercase_ratio=if(output_length>0, round(uppercase_count/output_length, 4), 0) \
| apply pii_detection_model \
| eval "gen_ai.pii.risk_score"=round(coalesce('probability(pii_label=1)', 'predicted(pii_label)'), 4) \
| eval "gen_ai.pii.ml_detected"=if('gen_ai.pii.risk_score'>0.5, "true", "false") \
| eval "gen_ai.pii.confidence"=case( \
    'gen_ai.pii.risk_score'>0.9, "very_high", \
    'gen_ai.pii.risk_score'>0.7, "high", \
    'gen_ai.pii.risk_score'>0.5, "medium", \
    'gen_ai.pii.risk_score'>0.3, "low", \
    1=1, "very_low" \
) \
| eval pii_types_detected=mvappend( \
    if(has_ssn=1, "SSN", null()), \
    if(has_email=1, "EMAIL", null()), \
    if(has_phone=1, "PHONE", null()), \
    if(has_dob=1, "DOB", null()), \
    if(has_address=1, "ADDRESS", null()), \
    if(has_credit_card=1, "CREDIT_CARD", null()), \
    if(has_name=1, "NAME", null()) \
) \
| eval "gen_ai.pii.types"=mvjoin(pii_types_detected, ",") \
| eval _raw=json_object( \
    "timestamp", strftime(_time, "%Y-%m-%dT%H:%M:%S"), \
    "source", "pii_ml_scoring", \
    "gen_ai.event.id", 'gen_ai.event.id', \
    "gen_ai.request.id", 'gen_ai.request.id', \
    "gen_ai.session.id", 'gen_ai.session.id', \
    "gen_ai.app.name", 'gen_ai.app.name', \
    "gen_ai.request.model", 'gen_ai.request.model', \
    "gen_ai.pii.risk_score", 'gen_ai.pii.risk_score', \
    "gen_ai.pii.ml_detected", 'gen_ai.pii.ml_detected', \
    "gen_ai.pii.confidence", 'gen_ai.pii.confidence', \
    "gen_ai.pii.types", 'gen_ai.pii.types', \
    "client.address", 'client.address', \
    "service.name", 'service.name', \
    "trace_id", 'trace_id' \
) \
| collect index=gen_ai_log source="pii_ml_scoring" sourcetype="gen_ai:pii:scoring"
dispatch.earliest_time = -1m@m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
alert.track = 0

###############################################################################
# PII DETECTION ML MODEL - ALERTS
###############################################################################

[GenAI - PII ML High Risk Alert]
description = Alerts when ML model detects high PII risk score (>0.7) in AI responses
search = index=gen_ai_log \
    gen_ai.pii.risk_score>0.7 \
| stats count as high_risk_count, \
    avg(gen_ai.pii.risk_score) as avg_risk_score, \
    max(gen_ai.pii.risk_score) as max_risk_score, \
    values(gen_ai.pii.types) as pii_types, \
    dc(gen_ai.session.id) as affected_sessions, \
    dc(client.address) as unique_sources \
    by gen_ai.deployment.id, gen_ai.app.name \
| where high_risk_count >= 1
dispatch.earliest_time = -15m
dispatch.latest_time = now
cron_schedule = */15 * * * *
enableSched = 1
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
alert_type = number of events
alert_comparator = greater than
alert_threshold = 0
action.email = 1
action.email.to = privacy-team@example.com, ai-governance@example.com
action.email.subject = GenAI PII ML High Risk Detected - $result.gen_ai.app.name$
action.email.message.alert = ML-based PII detection has identified high-risk responses.\n\nApplication: $result.gen_ai.app.name$\nDeployment: $result.gen_ai.deployment.id$\nHigh Risk Events: $result.high_risk_count$\nMax Risk Score: $result.max_risk_score$\nPII Types: $result.pii_types$\nAffected Sessions: $result.affected_sessions$

[GenAI - PII ML Rate Threshold Alert]
description = Alerts when overall PII detection rate exceeds threshold
search = index=gen_ai_log \
    (gen_ai.pii.ml_detected="true" OR gen_ai.pii.ml_detected="false") \
| stats count as total_events, \
    sum(eval(if('gen_ai.pii.ml_detected'="true", 1, 0))) as pii_events, \
    avg(gen_ai.pii.risk_score) as avg_risk_score \
    by gen_ai.deployment.id, gen_ai.app.name \
| eval pii_rate=round((pii_events/total_events)*100, 2) \
| where pii_rate > 5 OR avg_risk_score > 0.4 \
| eval alert_message=case( \
    pii_rate > 10, "CRITICAL: PII rate of ".pii_rate."% exceeds 10% threshold", \
    pii_rate > 5, "WARNING: PII rate of ".pii_rate."% exceeds 5% threshold", \
    avg_risk_score > 0.5, "WARNING: Average risk score ".round(avg_risk_score, 2)." is elevated", \
    1=1, "PII rate or risk score above threshold" \
)
dispatch.earliest_time = -4h
dispatch.latest_time = now
cron_schedule = 0 */4 * * *
enableSched = 1
alert.track = 1
alert.severity = 4
alert_type = always

###############################################################################
# PII DETECTION ML MODEL - REPORTS
###############################################################################

[GenAI - PII ML Daily Summary Report]
description = Daily summary of PII detection results from ML model
search = index=gen_ai_log earliest=-24h latest=now \
| eval response_text='gen_ai.output.messages' \
| where isnotnull(response_text) AND len(response_text) > 20 \
| eval output_length=len(response_text) \
| eval word_count=mvcount(split(response_text, " ")) \
| rex field=response_text "(?<ssn_match>\d{3}-\d{2}-\d{4})" \
| eval has_ssn=if(isnotnull(ssn_match), 1, 0) \
| rex field=response_text "(?<email_match>[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})" \
| eval has_email=if(isnotnull(email_match), 1, 0) \
| rex field=response_text "(?<phone_match>\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})" \
| eval has_phone=if(isnotnull(phone_match), 1, 0) \
| rex field=response_text "(?<dob_match>(?:date of birth|DOB|born):?\s*\d{1,2}[/-]\d{1,2}[/-]\d{2,4})" \
| eval has_dob=if(isnotnull(dob_match), 1, 0) \
| rex field=response_text "(?<address_match>\d+\s+[A-Za-z]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Court|Ct|Place|Pl),?\s+[A-Z][a-z]+,?\s+[A-Z]{2}\s+\d{5})" \
| eval has_address=if(isnotnull(address_match), 1, 0) \
| rex field=response_text "(?<cc_match>\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4})" \
| eval has_credit_card=if(isnotnull(cc_match), 1, 0) \
| rex field=response_text "(?<name_match>(?:patient|for|Hi|Mr\.|Mrs\.|Ms\.|Dr\.)\s+([A-Z][a-z]+\s+[A-Z][a-z]+))" \
| eval has_name=if(isnotnull(name_match), 1, 0) \
| eval digit_count=len(replace(response_text, "[^\d]", "")) \
| eval digit_ratio=if(output_length>0, round(digit_count/output_length, 4), 0) \
| eval special_char_ratio=if(output_length>0, round(len(replace(response_text, "[A-Za-z0-9\s]", ""))/output_length, 4), 0) \
| eval uppercase_ratio=if(output_length>0, round(len(replace(response_text, "[^A-Z]", ""))/output_length, 4), 0) \
| apply pii_detection_model \
| eval pii_risk_score=round(coalesce('probability(pii_label=1)', 'predicted(pii_label)'), 3) \
| eval pii_ml_detected=if(pii_risk_score>0.5, "true", "false") \
| stats count as total_events, \
    sum(eval(if(pii_ml_detected="true", 1, 0))) as total_pii_detected, \
    sum(eval(if(pii_risk_score>0.9, 1, 0))) as very_high_risk, \
    sum(eval(if(pii_risk_score>0.7 AND pii_risk_score<=0.9, 1, 0))) as high_risk, \
    sum(eval(if(pii_risk_score>0.5 AND pii_risk_score<=0.7, 1, 0))) as medium_risk, \
    avg(pii_risk_score) as avg_risk_score, \
    dc(gen_ai.session.id) as affected_sessions \
    by gen_ai.app.name \
| eval pii_rate=round((total_pii_detected/total_events)*100, 2)."%" \
| sort -total_pii_detected \
| rename gen_ai.app.name as Application
dispatch.earliest_time = -24h
dispatch.latest_time = now
cron_schedule = 0 8 * * *
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ai-governance@example.com, privacy-team@example.com
action.email.subject = Daily GenAI PII Detection Summary

[GenAI - PII ML Weekly Model Performance Report]
description = Weekly report on PII ML model performance and health
search = index=gen_ai_log \
    gen_ai.pii.risk_score=* \
| stats count as total_scored, \
    avg(gen_ai.pii.risk_score) as avg_score, \
    stdev(gen_ai.pii.risk_score) as stdev_score, \
    perc50(gen_ai.pii.risk_score) as median_score, \
    sum(eval(if('gen_ai.pii.ml_detected'="true", 1, 0))) as pii_detected, \
    sum(eval(if('gen_ai.pii.detected'="true", 1, 0))) as rule_detected \
| eval ml_detection_rate=round((pii_detected/total_scored)*100, 2) \
| eval model_health=case( \
    ml_detection_rate > 20, "WARNING - High detection rate, review for false positives", \
    ml_detection_rate < 1, "WARNING - Low detection rate, review model performance", \
    stdev_score > 0.4, "WARNING - High variance in scores", \
    1=1, "HEALTHY" \
) \
| table total_scored avg_score median_score stdev_score ml_detection_rate model_health
dispatch.earliest_time = -7d
dispatch.latest_time = now
cron_schedule = 0 9 * * 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ai-governance@example.com, ml-ops@example.com
action.email.subject = Weekly GenAI PII Model Performance Report

[GenAI - PII Types Distribution Report]
description = Weekly breakdown of PII types detected across applications
search = index=gen_ai_log \
    gen_ai.pii.ml_detected="true" \
    gen_ai.pii.types=* \
| mvexpand gen_ai.pii.types \
| stats count as occurrences, \
    dc(gen_ai.session.id) as unique_sessions, \
    dc(gen_ai.app.name) as apps_affected \
    by gen_ai.pii.types \
| sort -occurrences \
| head 20
dispatch.earliest_time = -7d
dispatch.latest_time = now
cron_schedule = 0 10 * * 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ai-governance@example.com
action.email.subject = Weekly GenAI PII Types Distribution Report

###############################################################################
# PROMPT INJECTION DETECTION ML MODEL - TRAINING
###############################################################################

[GenAI - Prompt Injection Train Step 1 - Feature Engineering from Initial Dataset]
description = Step 1 of 3: Prepare feature-engineered training data from prompt injection dataset. Run this FIRST before training.
search = | inputlookup prompt_injection_training_examples \
| eval input_text=prompt \
| eval injection_label=coalesce(injection_label, 0) \
| where len(input_text) > 0 \
| eval prompt_length=len(input_text) \
| eval word_count=mvcount(split(input_text, " ")) \
| eval has_ignore_instruction=if(match(input_text, "(?i)(ignore|disregard|forget).*(previous|prior|above|earlier).*(instruction|prompt|rule|guideline)"), 1, 0) \
| eval has_reveal_request=if(match(input_text, "(?i)(reveal|show|display|tell me|what is).*(system|original|hidden).*(prompt|instruction|rule)"), 1, 0) \
| eval has_bypass_request=if(match(input_text, "(?i)(bypass|override|disable|turn off).*(safety|filter|guardrail|restriction|limit)"), 1, 0) \
| eval has_roleplay_injection=if(match(input_text, "(?i)(pretend|act as|imagine you are|you are now|from now on).*(unrestricted|unfiltered|without limits|no rules)"), 1, 0) \
| eval has_jailbreak_terms=if(match(input_text, "(?i)(jailbreak|DAN mode|sudo mode|developer mode|god mode|admin mode|STAN|DUDE|AntiGPT)"), 1, 0) \
| eval has_encoding=if(match(input_text, "(\\\\x[0-9a-fA-F]{2}|%[0-9a-fA-F]{2}|&#\d+;|&#x[0-9a-fA-F]+;|base64|rot13|decode)"), 1, 0) \
| eval special_char_count=len(replace(input_text, "[A-Za-z0-9\s]", "")) \
| eval special_char_ratio=if(prompt_length>0, round(special_char_count/prompt_length, 4), 0) \
| rex field=input_text max_match=100 "(?i)(?<negation_match>don't|do not|never|not|no|none)" \
| eval negation_count=if(isnull(negation_match), 0, mvcount(negation_match)) \
| eval negation_density=if(word_count>0, round(negation_count/word_count, 4), 0) \
| eval starts_with_command=if(match(input_text, "(?i)^(ignore|disregard|forget|reveal|show|tell|bypass|override|enable|activate|switch|enter|turn)"), 1, 0) \
| table input_text injection_label technique prompt_length word_count special_char_ratio negation_density \
    has_ignore_instruction has_reveal_request has_bypass_request has_roleplay_injection has_jailbreak_terms has_encoding starts_with_command \
| outputlookup prompt_injection_training_data_engineered \
| stats count as total_rows, \
    sum(injection_label) as attack_examples, \
    sum(eval(if(injection_label=0,1,0))) as clean_examples, \
    sum(has_ignore_instruction) as ignore_detected, \
    sum(has_reveal_request) as reveal_detected, \
    sum(has_bypass_request) as bypass_detected, \
    sum(has_roleplay_injection) as roleplay_detected, \
    sum(has_jailbreak_terms) as jailbreak_detected, \
    sum(has_encoding) as encoding_detected \
| eval status="SUCCESS: Feature engineering complete" \
| eval attack_percentage=round((attack_examples/total_rows)*100, 2)."%" \
| table status total_rows attack_examples clean_examples attack_percentage ignore_detected reveal_detected bypass_detected roleplay_detected jailbreak_detected encoding_detected
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - Prompt Injection Train Step 2 - Random Forest Model]
description = Step 2 of 3: Train Random Forest model for prompt injection detection. Run AFTER Step 1 completes.
search = | inputlookup prompt_injection_training_data_engineered \
| fit RandomForestClassifier injection_label \
    from prompt_length word_count special_char_ratio negation_density \
    has_ignore_instruction has_reveal_request has_bypass_request \
    has_roleplay_injection has_jailbreak_terms has_encoding starts_with_command \
    max_depth=15 \
    max_features=8 \
    n_estimators=100 \
    random_state=42 \
    into app:prompt_injection_model
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - Prompt Injection Train Step 2 Alt - Logistic Regression Model]
description = Alternative Step 2: Train Logistic Regression model (faster, more interpretable). Run AFTER Step 1 completes.
search = | inputlookup prompt_injection_training_data_engineered \
| fit LogisticRegression injection_label \
    from prompt_length word_count special_char_ratio negation_density \
    has_ignore_instruction has_reveal_request has_bypass_request \
    has_roleplay_injection has_jailbreak_terms has_encoding starts_with_command \
    into app:prompt_injection_model
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

[GenAI - Prompt Injection Train Step 3 - Validate Model Performance]
description = Step 3 of 3: Validate model performance metrics. Run AFTER Step 2 completes.
search = | inputlookup prompt_injection_training_data_engineered \
| sample 1000 \
| apply prompt_injection_model \
| eval predicted_injection='predicted(injection_label)' \
| eval predicted_class=if(predicted_injection>0.6, 1, 0) \
| eval true_positive=if(injection_label=1 AND predicted_class=1, 1, 0) \
| eval true_negative=if(injection_label=0 AND predicted_class=0, 1, 0) \
| eval false_positive=if(injection_label=0 AND predicted_class=1, 1, 0) \
| eval false_negative=if(injection_label=1 AND predicted_class=0, 1, 0) \
| stats sum(true_positive) as TP, \
    sum(true_negative) as TN, \
    sum(false_positive) as FP, \
    sum(false_negative) as FN \
| eval Total=TP+TN+FP+FN \
| eval Accuracy=round((TP+TN)/Total, 4) \
| eval Precision=round(TP/(TP+FP), 4) \
| eval Recall=round(TP/(TP+FN), 4) \
| eval F1_Score=round(2*(Precision*Recall)/(Precision+Recall), 4) \
| eval Specificity=round(TN/(TN+FP), 4) \
| table Accuracy Precision Recall F1_Score Specificity TP TN FP FN Total
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
alert.track = 0

###############################################################################
# PROMPT INJECTION DETECTION ML MODEL - SCHEDULED SCORING
###############################################################################

[GenAI - Prompt Injection Scoring - Prompt Analysis]
description = Apply prompt injection detection model to score AI prompts every minute
search = index=gen_ai_log NOT sourcetype="gen_ai:prompt_injection:scoring" earliest=-1m@m latest=now \
| eval input_text=coalesce('gen_ai.input.messages', 'event.input', input) \
| where isnotnull(input_text) AND len(input_text) > 10 \
| dedup gen_ai.event.id \
| eval prompt_length=len(input_text) \
| eval word_count=mvcount(split(input_text, " ")) \
| eval has_ignore_instruction=if(match(input_text, "(?i)(ignore|disregard|forget).*(previous|prior|above|earlier).*(instruction|prompt|rule|guideline)"), 1, 0) \
| eval has_reveal_request=if(match(input_text, "(?i)(reveal|show|display|tell me|what is).*(system|original|hidden).*(prompt|instruction|rule)"), 1, 0) \
| eval has_bypass_request=if(match(input_text, "(?i)(bypass|override|disable|turn off).*(safety|filter|guardrail|restriction|limit)"), 1, 0) \
| eval has_roleplay_injection=if(match(input_text, "(?i)(pretend|act as|imagine you are|you are now|from now on).*(unrestricted|unfiltered|without limits|no rules)"), 1, 0) \
| eval has_jailbreak_terms=if(match(input_text, "(?i)(jailbreak|DAN mode|sudo mode|developer mode|god mode|admin mode|STAN|DUDE|AntiGPT)"), 1, 0) \
| eval has_encoding=if(match(input_text, "(\\\\x[0-9a-fA-F]{2}|%[0-9a-fA-F]{2}|&#\d+;|&#x[0-9a-fA-F]+;|base64|rot13|decode)"), 1, 0) \
| eval special_char_count=len(replace(input_text, "[A-Za-z0-9\s]", "")) \
| eval special_char_ratio=if(prompt_length>0, round(special_char_count/prompt_length, 4), 0) \
| rex field=input_text max_match=100 "(?i)(?<negation_match>don't|do not|never|not|no|none)" \
| eval negation_count=if(isnull(negation_match), 0, mvcount(negation_match)) \
| eval negation_density=if(word_count>0, round(negation_count/word_count, 4), 0) \
| eval starts_with_command=if(match(input_text, "(?i)^(ignore|disregard|forget|reveal|show|tell|bypass|override|enable|activate|switch|enter|turn)"), 1, 0) \
| apply prompt_injection_model \
| eval "gen_ai.prompt_injection.risk_score"=round('predicted(injection_label)', 4) \
| eval "gen_ai.prompt_injection.ml_detected"=if('gen_ai.prompt_injection.risk_score'>0.6, "true", "false") \
| eval "gen_ai.prompt_injection.confidence"=case( \
    'gen_ai.prompt_injection.risk_score'>0.9, "very_high", \
    'gen_ai.prompt_injection.risk_score'>0.8, "high", \
    'gen_ai.prompt_injection.risk_score'>0.6, "medium", \
    'gen_ai.prompt_injection.risk_score'>0.4, "low", \
    1=1, "very_low" \
) \
| eval "gen_ai.prompt_injection.technique"=case( \
    has_ignore_instruction=1, "ignore_instructions", \
    has_reveal_request=1, "reveal_system", \
    has_bypass_request=1, "bypass_safety", \
    has_roleplay_injection=1, "roleplay_injection", \
    has_jailbreak_terms=1, "jailbreak", \
    has_encoding=1, "encoding_obfuscation", \
    1=1, "unknown" \
) \
| eval "gen_ai.prompt_injection.severity"=case( \
    'gen_ai.prompt_injection.risk_score'>0.9, "critical", \
    'gen_ai.prompt_injection.risk_score'>0.7, "high", \
    'gen_ai.prompt_injection.risk_score'>0.5, "medium", \
    'gen_ai.prompt_injection.risk_score'>0.3, "low", \
    1=1, "none" \
) \
| eval _raw=json_object( \
    "timestamp", strftime(_time, "%Y-%m-%dT%H:%M:%S"), \
    "source", "prompt_injection_ml_scoring", \
    "gen_ai.event.id", 'gen_ai.event.id', \
    "gen_ai.request.id", 'gen_ai.request.id', \
    "gen_ai.session.id", 'gen_ai.session.id', \
    "gen_ai.app.name", 'gen_ai.app.name', \
    "gen_ai.request.model", 'gen_ai.request.model', \
    "gen_ai.prompt_injection.risk_score", 'gen_ai.prompt_injection.risk_score', \
    "gen_ai.prompt_injection.ml_detected", 'gen_ai.prompt_injection.ml_detected', \
    "gen_ai.prompt_injection.confidence", 'gen_ai.prompt_injection.confidence', \
    "gen_ai.prompt_injection.technique", 'gen_ai.prompt_injection.technique', \
    "gen_ai.prompt_injection.severity", 'gen_ai.prompt_injection.severity', \
    "client.address", 'client.address', \
    "service.name", 'service.name', \
    "trace_id", 'trace_id' \
) \
| collect index=gen_ai_log source="prompt_injection_ml_scoring" sourcetype="gen_ai:prompt_injection:scoring"
dispatch.earliest_time = -1m@m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
alert.track = 0

###############################################################################
# PROMPT INJECTION DETECTION ML MODEL - REPORTS
###############################################################################

[GenAI - Prompt Injection ML Daily Summary Report]
description = Daily summary of prompt injection detection results from ML model
search = index=gen_ai_log sourcetype="gen_ai:prompt_injection:scoring" earliest=-24h latest=now \
| stats count as total_scored, \
    sum(eval(if('gen_ai.prompt_injection.ml_detected'="true", 1, 0))) as injections_detected, \
    avg(gen_ai.prompt_injection.risk_score) as avg_risk_score, \
    max(gen_ai.prompt_injection.risk_score) as max_risk_score, \
    dc(gen_ai.session.id) as unique_sessions, \
    dc(client.address) as unique_sources \
    by gen_ai.app.name \
| eval detection_rate=round((injections_detected/total_scored)*100, 2) \
| sort -injections_detected
dispatch.earliest_time = -24h
dispatch.latest_time = now
cron_schedule = 0 8 * * *
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = Daily GenAI Prompt Injection Detection Summary

[GenAI - Prompt Injection ML Technique Breakdown Report]
description = Weekly report breaking down prompt injection attempts by technique type
search = index=gen_ai_log sourcetype="gen_ai:prompt_injection:scoring" \
    gen_ai.prompt_injection.ml_detected="true" earliest=-7d latest=now \
| stats count as attempts, \
    avg(gen_ai.prompt_injection.risk_score) as avg_risk, \
    dc(client.address) as unique_sources, \
    dc(gen_ai.session.id) as unique_sessions \
    by gen_ai.prompt_injection.technique \
| sort -attempts
dispatch.earliest_time = -7d
dispatch.latest_time = now
cron_schedule = 0 9 * * 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = Weekly GenAI Prompt Injection Technique Breakdown

###############################################################################
# PII DETECTION - AUTOMATIC REVIEW QUEUE ESCALATION
###############################################################################

[GenAI - Auto Escalate PII to Review Queue]
description = Scheduled alert that automatically adds PII detections to the AI Governance Review Queue. Uses _indextime to prevent missed events due to indexing delays.
search = index=gen_ai_log _index_earliest=-1m@m _index_latest=now gen_ai.pii.ml_detected="true" \
| where isnotnull('gen_ai.event.id') AND 'gen_ai.event.id'!="" AND 'gen_ai.event.id'!="null" \
| dedup gen_ai.event.id \
| eval pii_risk_score='gen_ai.pii.risk_score', \
    pii_confidence='gen_ai.pii.confidence', \
    pii_types_detected='gen_ai.pii.types', \
    trace_id_from_scoring='gen_ai.trace.id' \
| join type=left gen_ai.event.id \
    [search index=gen_ai_log _index_earliest=-5m _index_latest=now gen_ai.output.messages=* \
    | dedup gen_ai.event.id \
    | table gen_ai.event.id, gen_ai.app.name, gen_ai.request.model, gen_ai.input.messages, gen_ai.output.messages, gen_ai.trace.id] \
| eval _key='gen_ai.event.id' \
| eval gen_ai_event_id='gen_ai.event.id' \
| lookup gen_ai_review_findings_lookup gen_ai_event_id OUTPUT gen_ai_review_status AS existing_status \
| where isnull(existing_status) \
| eval gen_ai_trace_id=coalesce('gen_ai.trace.id', trace_id_from_scoring, ""), \
    event_time=_time, \
    gen_ai_app_name=coalesce('gen_ai.app.name', "Unknown"), \
    gen_ai_request_model=coalesce('gen_ai.request.model', "Unknown"), \
    gen_ai_input_preview=substr(coalesce('gen_ai.input.messages', ""), 1, 200), \
    gen_ai_output_preview=substr(coalesce('gen_ai.output.messages', ""), 1, 200), \
    gen_ai_review_reviewer="", \
    gen_ai_review_assignee="", \
    gen_ai_review_status="new", \
    gen_ai_review_priority=case( \
        match(pii_types_detected, "(?i)SSN|CREDIT_CARD"), "critical", \
        pii_risk_score>0.7, "high", \
        pii_risk_score>0.4, "medium", \
        1=1, "low" \
    ), \
    gen_ai_review_pii_confirmed="n/a", \
    gen_ai_review_pii_types=coalesce(pii_types_detected, ""), \
    gen_ai_review_phi_confirmed="false", \
    gen_ai_review_phi_types="", \
    gen_ai_review_prompt_injection_confirmed="false", \
    gen_ai_review_prompt_injection_type="", \
    gen_ai_review_anomaly_prompt_detected="n/a", \
    gen_ai_review_anomaly_prompt_reviewed="n/a", \
    gen_ai_review_anomaly_response_detected="n/a", \
    gen_ai_review_anomaly_response_reviewed="n/a", \
    gen_ai_review_notes="Auto-escalated by PII ML detection. Risk score: ".coalesce(pii_risk_score, "N/A").", Confidence: ".coalesce(pii_confidence, "N/A").", Types: ".coalesce(pii_types_detected, "N/A"), \
    gen_ai_review_created_at=now(), \
    gen_ai_review_updated_at=now(), \
    gen_ai_review_created_by="system_pii_detection", \
    gen_ai_review_updated_by="system_pii_detection" \
| table _key, gen_ai_event_id, gen_ai_trace_id, event_time, gen_ai_app_name, gen_ai_request_model, gen_ai_input_preview, gen_ai_output_preview, gen_ai_review_reviewer, gen_ai_review_assignee, gen_ai_review_status, gen_ai_review_priority, gen_ai_review_pii_confirmed, gen_ai_review_pii_types, gen_ai_review_phi_confirmed, gen_ai_review_phi_types, gen_ai_review_prompt_injection_confirmed, gen_ai_review_prompt_injection_type, gen_ai_review_anomaly_prompt_detected, gen_ai_review_anomaly_prompt_reviewed, gen_ai_review_anomaly_response_detected, gen_ai_review_anomaly_response_reviewed, gen_ai_review_notes, gen_ai_review_created_at, gen_ai_review_updated_at, gen_ai_review_created_by, gen_ai_review_updated_by \
| outputlookup append=true gen_ai_review_findings_lookup
dispatch.earliest_time = -10m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 4
alert.suppress = 1
alert.suppress.fields = event_id
alert.suppress.period = 24h

[GenAI - Auto Escalate Random Sample to Review Queue]
description = Scheduled alert that randomly escalates events to the AI Governance Review Queue for quality assurance when Random Escalation is enabled. Events are selected when gen_ai.pii.ml_detected=false and the final alphanumeric characters of gen_ai.event.id match the configured RNG Seed. Uses _indextime to prevent missed events due to indexing delays.
search = | rest /servicesNS/nobody/TA-gen_ai_cim/configs/conf-ta_gen_ai_cim_detection/settings splunk_server=local \
| where random_escalation="1" AND isnotnull(rng_seed) AND rng_seed!="" \
| fields rng_seed \
| head 1 \
| map maxsearches=1 search="search index=gen_ai_log _index_earliest=-1m@m _index_latest=now gen_ai.pii.ml_detected=\"false\" \
| where isnotnull('gen_ai.event.id') AND 'gen_ai.event.id'!=\"\" AND 'gen_ai.event.id'!=\"null\" \
| dedup gen_ai.event.id \
| eval event_id_alphanumeric=replace('gen_ai.event.id', \"[^a-zA-Z0-9]\", \"\") \
| eval rng_seed_value=\"$$rng_seed$$\" \
| eval seed_len=len(rng_seed_value) \
| where seed_len > 0 \
| eval event_id_suffix=substr(event_id_alphanumeric, len(event_id_alphanumeric) - seed_len + 1, seed_len) \
| where lower(event_id_suffix)=lower(rng_seed_value) \
| eval pii_risk_score='gen_ai.pii.risk_score', \
    pii_confidence='gen_ai.pii.confidence', \
    pii_types_detected='gen_ai.pii.types', \
    trace_id_from_scoring='gen_ai.trace.id' \
| join type=left gen_ai.event.id \
    [search index=gen_ai_log _index_earliest=-5m _index_latest=now gen_ai.output.messages=* \
    | dedup gen_ai.event.id \
    | table gen_ai.event.id, gen_ai.app.name, gen_ai.request.model, gen_ai.input.messages, gen_ai.output.messages, gen_ai.trace.id] \
| eval _key='gen_ai.event.id' \
| eval gen_ai_event_id='gen_ai.event.id' \
| lookup gen_ai_review_findings_lookup gen_ai_event_id OUTPUT gen_ai_review_status AS existing_status \
| where isnull(existing_status) \
| eval gen_ai_trace_id=coalesce('gen_ai.trace.id', trace_id_from_scoring, \"\"), \
    event_time=_time, \
    gen_ai_app_name=coalesce('gen_ai.app.name', \"Unknown\"), \
    gen_ai_request_model=coalesce('gen_ai.request.model', \"Unknown\"), \
    gen_ai_input_preview=substr(coalesce('gen_ai.input.messages', \"\"), 1, 200), \
    gen_ai_output_preview=substr(coalesce('gen_ai.output.messages', \"\"), 1, 200), \
    gen_ai_review_reviewer=\"\", \
    gen_ai_review_assignee=\"\", \
    gen_ai_review_status=\"new\", \
    gen_ai_review_priority=\"low\", \
    gen_ai_review_pii_confirmed=\"n/a\", \
    gen_ai_review_pii_types=coalesce(pii_types_detected, \"\"), \
    gen_ai_review_phi_confirmed=\"false\", \
    gen_ai_review_phi_types=\"\", \
    gen_ai_review_prompt_injection_confirmed=\"false\", \
    gen_ai_review_prompt_injection_type=\"\", \
    gen_ai_review_anomaly_prompt_detected=\"n/a\", \
    gen_ai_review_anomaly_prompt_reviewed=\"n/a\", \
    gen_ai_review_anomaly_response_detected=\"n/a\", \
    gen_ai_review_anomaly_response_reviewed=\"n/a\", \
    gen_ai_review_notes=\"Auto-escalated by random sampling (RNG Seed match). Event ID suffix matched: \".event_id_suffix.\". PII ML detected: false. Risk score: \".coalesce(pii_risk_score, \"N/A\"), \
    gen_ai_review_created_at=now(), \
    gen_ai_review_updated_at=now(), \
    gen_ai_review_created_by=\"system_random_escalation\", \
    gen_ai_review_updated_by=\"system_random_escalation\" \
| table _key, gen_ai_event_id, gen_ai_trace_id, event_time, gen_ai_app_name, gen_ai_request_model, gen_ai_input_preview, gen_ai_output_preview, gen_ai_review_reviewer, gen_ai_review_assignee, gen_ai_review_status, gen_ai_review_priority, gen_ai_review_pii_confirmed, gen_ai_review_pii_types, gen_ai_review_phi_confirmed, gen_ai_review_phi_types, gen_ai_review_prompt_injection_confirmed, gen_ai_review_prompt_injection_type, gen_ai_review_anomaly_prompt_detected, gen_ai_review_anomaly_prompt_reviewed, gen_ai_review_anomaly_response_detected, gen_ai_review_anomaly_response_reviewed, gen_ai_review_notes, gen_ai_review_created_at, gen_ai_review_updated_at, gen_ai_review_created_by, gen_ai_review_updated_by \
| outputlookup append=true gen_ai_review_findings_lookup"
dispatch.earliest_time = -10m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
alert.suppress = 1
alert.suppress.fields = event_id
alert.suppress.period = 24h

###############################################################################
# PROMPT INJECTION DETECTION - AUTOMATIC REVIEW QUEUE ESCALATION
###############################################################################

[GenAI - Auto Escalate Prompt Injection to Review Queue]
description = Scheduled alert that automatically adds Prompt Injection detections to the AI Governance Review Queue. Uses _indextime to prevent missed events due to indexing delays.
search = index=gen_ai_log _index_earliest=-1m@m _index_latest=now gen_ai.prompt_injection.ml_detected="true" \
| where isnotnull('gen_ai.event.id') AND 'gen_ai.event.id'!="" AND 'gen_ai.event.id'!="null" \
| dedup gen_ai.event.id \
| eval injection_risk_score='gen_ai.prompt_injection.risk_score', \
    injection_confidence='gen_ai.prompt_injection.confidence', \
    injection_severity='gen_ai.prompt_injection.severity', \
    injection_technique='gen_ai.prompt_injection.technique', \
    trace_id_from_scoring='gen_ai.trace.id' \
| join type=left gen_ai.event.id \
    [search index=gen_ai_log _index_earliest=-5m _index_latest=now gen_ai.output.messages=* \
    | dedup gen_ai.event.id \
    | table gen_ai.event.id, gen_ai.app.name, gen_ai.request.model, gen_ai.input.messages, gen_ai.output.messages, gen_ai.trace.id] \
| eval _key='gen_ai.event.id' \
| eval gen_ai_event_id='gen_ai.event.id' \
| lookup gen_ai_review_findings_lookup gen_ai_event_id OUTPUT gen_ai_review_status AS existing_status \
| where isnull(existing_status) \
| eval gen_ai_trace_id=coalesce('gen_ai.trace.id', trace_id_from_scoring, ""), \
    event_time=_time, \
    gen_ai_app_name=coalesce('gen_ai.app.name', "Unknown"), \
    gen_ai_request_model=coalesce('gen_ai.request.model', "Unknown"), \
    gen_ai_input_preview=substr(coalesce('gen_ai.input.messages', ""), 1, 200), \
    gen_ai_output_preview=substr(coalesce('gen_ai.output.messages', ""), 1, 200), \
    gen_ai_review_reviewer="", \
    gen_ai_review_assignee="", \
    gen_ai_review_status="new", \
    gen_ai_review_priority=case( \
        injection_severity="critical" OR injection_risk_score>0.9, "critical", \
        injection_severity="high" OR injection_risk_score>0.7, "high", \
        injection_severity="medium" OR injection_risk_score>0.5, "medium", \
        1=1, "low" \
    ), \
    gen_ai_review_pii_confirmed="n/a", \
    gen_ai_review_pii_types="", \
    gen_ai_review_phi_confirmed="false", \
    gen_ai_review_phi_types="", \
    gen_ai_review_prompt_injection_confirmed="false", \
    gen_ai_review_prompt_injection_type=coalesce(injection_technique, ""), \
    gen_ai_review_anomaly_prompt_detected="n/a", \
    gen_ai_review_anomaly_prompt_reviewed="n/a", \
    gen_ai_review_anomaly_response_detected="n/a", \
    gen_ai_review_anomaly_response_reviewed="n/a", \
    gen_ai_review_notes="Auto-escalated by Prompt Injection ML detection. Risk score: ".coalesce(injection_risk_score, "N/A").", Confidence: ".coalesce(injection_confidence, "N/A").", Severity: ".coalesce(injection_severity, "N/A").", Technique: ".coalesce(injection_technique, "N/A"), \
    gen_ai_review_created_at=now(), \
    gen_ai_review_updated_at=now(), \
    gen_ai_review_created_by="system_prompt_injection_detection", \
    gen_ai_review_updated_by="system_prompt_injection_detection" \
| table _key, gen_ai_event_id, gen_ai_trace_id, event_time, gen_ai_app_name, gen_ai_request_model, gen_ai_input_preview, gen_ai_output_preview, gen_ai_review_reviewer, gen_ai_review_assignee, gen_ai_review_status, gen_ai_review_priority, gen_ai_review_pii_confirmed, gen_ai_review_pii_types, gen_ai_review_phi_confirmed, gen_ai_review_phi_types, gen_ai_review_prompt_injection_confirmed, gen_ai_review_prompt_injection_type, gen_ai_review_anomaly_prompt_detected, gen_ai_review_anomaly_prompt_reviewed, gen_ai_review_anomaly_response_detected, gen_ai_review_anomaly_response_reviewed, gen_ai_review_notes, gen_ai_review_created_at, gen_ai_review_updated_at, gen_ai_review_created_by, gen_ai_review_updated_by \
| outputlookup append=true gen_ai_review_findings_lookup
dispatch.earliest_time = -10m
dispatch.latest_time = now
cron_schedule = * * * * *
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 5
alert.suppress = 1
alert.suppress.fields = event_id
alert.suppress.period = 24h

###############################################################################
# REVIEW WORKFLOW SAVED SEARCHES
###############################################################################

[GenAI - Unreviewed High Risk Events]
description = High-risk events that have not yet been reviewed by an analyst
search = index=gen_ai_log \
    (gen_ai.pii.detected=true OR gen_ai.safety.violated=true OR gen_ai.guardrail.triggered=true OR gen_ai.policy.blocked=true) \
| eval gen_ai_event_id=coalesce('gen_ai.event.id', 'gen_ai.response.id', md5(_time.'gen_ai.trace.id')) \
| lookup gen_ai_review_findings_lookup gen_ai_event_id OUTPUT gen_ai_review_status AS review_status \
| where isnull(review_status) OR review_status="new" OR review_status="unreviewed" \
| eval risk_score = (if('gen_ai.pii.detected'="true", 30, 0) + if('gen_ai.safety.violated'="true", 40, 0) + if('gen_ai.guardrail.triggered'="true", 20, 0) + if('gen_ai.policy.blocked'="true", 10, 0)) \
| sort - risk_score, - _time \
| table _time, gen_ai_event_id, gen_ai.app.name, gen_ai.request.model, gen_ai.pii.detected, gen_ai.safety.violated, gen_ai.guardrail.triggered, risk_score
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
cron_schedule = 0 6 * * *
enableSched = 1
is_visible = true
alert.track = 1
counttype = number of events
quantity = 50
relation = greater than
alert.severity = 3
alert.suppress = 0

###############################################################################
# PII FEEDBACK LOOP - ACTIVE LEARNING SEARCHES
# These searches implement the active learning feedback loop for PII detection
###############################################################################

[GenAI - PII Feedback Loop - Extract Training Feedback]
description = Daily extraction of human-labeled reviews into training feedback collection. Uses reviewer-confirmed PII types (gen_ai_review_pii_types_reviewed) as ground truth labels. Generates per-type labels for granular model training, classifies feedback type by comparing ML predictions with reviewer confirmations, assigns train/valid/test splits, and stores for model retraining.
search = | inputlookup gen_ai_review_findings_lookup \
| search gen_ai_review_status="completed" \
| eval reviewed_pii_types=coalesce(gen_ai_review_pii_types_reviewed, "") \
| eval detected_pii_types=coalesce(gen_ai_review_pii_types, "") \
| eval pii_label=if(len(reviewed_pii_types)>0, 1, if(gen_ai_review_pii_confirmed="true" OR gen_ai_review_pii_confirmed="TRUE" OR gen_ai_review_pii_confirmed="1" OR gen_ai_review_phi_confirmed="true" OR gen_ai_review_phi_confirmed="1", 1, 0)) \
| eval has_ssn_label=if(match(reviewed_pii_types, "(?i)SSN"), 1, 0) \
| eval has_email_label=if(match(reviewed_pii_types, "(?i)EMAIL"), 1, 0) \
| eval has_phone_label=if(match(reviewed_pii_types, "(?i)PHONE"), 1, 0) \
| eval has_dob_label=if(match(reviewed_pii_types, "(?i)DOB"), 1, 0) \
| eval has_address_label=if(match(reviewed_pii_types, "(?i)ADDRESS"), 1, 0) \
| eval has_credit_card_label=if(match(reviewed_pii_types, "(?i)CREDIT_CARD"), 1, 0) \
| eval has_name_label=if(match(reviewed_pii_types, "(?i)NAME"), 1, 0) \
| join type=left gen_ai_event_id \
    [search index=gen_ai_log gen_ai.output.messages=* earliest=-90d \
    | dedup gen_ai.event.id \
    | rename gen_ai.event.id AS gen_ai_event_id \
    | eval response_text='gen_ai.output.messages' \
    | table gen_ai_event_id, response_text, gen_ai.pii.risk_score, gen_ai.pii.ml_detected, gen_ai.pii.types, gen_ai.app.name, gen_ai.trace.id] \
| where isnotnull(response_text) AND len(response_text) > 20 \
| lookup pii_training_feedback_lookup event_id AS gen_ai_event_id OUTPUT event_id AS existing_event \
| where isnull(existing_event) \
| eval ml_predicted_score=coalesce('gen_ai.pii.risk_score', 0) \
| eval ml_predicted_label=coalesce('gen_ai.pii.ml_detected', "false") \
| eval ml_predicted_types=coalesce('gen_ai.pii.types', detected_pii_types, "") \
| eval feedback_type=case( \
    pii_label=1 AND ml_predicted_label="true" AND reviewed_pii_types=ml_predicted_types, "confirmed_pii_exact", \
    pii_label=1 AND ml_predicted_label="true", "confirmed_pii_type_mismatch", \
    pii_label=1 AND ml_predicted_label="false", "false_negative", \
    pii_label=0 AND ml_predicted_label="true", "false_positive", \
    pii_label=0 AND ml_predicted_label="false", "confirmed_clean" \
) \
| eval random_val=random()%100 \
| eval split_assignment=case(random_val<70, "train", random_val<85, "valid", 1=1, "test") \
| eval output_length=len(response_text) \
| eval word_count=mvcount(split(response_text, " ")) \
| rex field=response_text "(?<ssn_match>\d{3}-\d{2}-\d{4})" \
| eval has_ssn=if(isnotnull(ssn_match), 1, 0) \
| rex field=response_text "(?<email_match>[a-zA-Z0-9][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})" \
| eval has_email=if(isnotnull(email_match), 1, 0) \
| rex field=response_text "(?<phone_match>(\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})|(\d{3}\.\d{3}\.\d{4}))" \
| eval has_phone=if(isnotnull(phone_match), 1, 0) \
| rex field=response_text "(?<dob_match>(?:date of birth|DOB|born):?\s*(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}))" \
| eval has_dob=if(isnotnull(dob_match), 1, 0) \
| rex field=response_text "(?<address_match>\d+\s+[A-Za-z]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Way|Court|Ct|Place|Pl),?\s+[A-Z][a-z]+,?\s+[A-Z]{2}\s+\d{5})" \
| eval has_address=if(isnotnull(address_match), 1, 0) \
| rex field=response_text "(?<cc_match>\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4})" \
| eval has_credit_card=if(isnotnull(cc_match), 1, 0) \
| rex field=response_text "(?<name_match>(?:patient|for|Hi|Mr\.|Mrs\.|Ms\.|Dr\.)\s+([A-Z][a-z]+\s+[A-Z][a-z]+))" \
| eval has_name=if(isnotnull(name_match), 1, 0) \
| eval digit_count=len(replace(response_text, "[^\d]", "")) \
| eval digit_ratio=if(output_length>0, round(digit_count/output_length, 4), 0) \
| eval special_char_count=len(replace(response_text, "[A-Za-z0-9\s]", "")) \
| eval special_char_ratio=if(output_length>0, round(special_char_count/output_length, 4), 0) \
| eval uppercase_count=len(replace(response_text, "[^A-Z]", "")) \
| eval uppercase_ratio=if(output_length>0, round(uppercase_count/output_length, 4), 0) \
| eval _key=gen_ai_event_id \
| eval event_id=gen_ai_event_id \
| eval trace_id=coalesce('gen_ai.trace.id', "") \
| eval response_length=output_length \
| eval pii_types=coalesce(detected_pii_types, "") \
| eval pii_types_reviewed=reviewed_pii_types \
| eval phi_confirmed=if(gen_ai_review_phi_confirmed="true" OR gen_ai_review_phi_confirmed="1", "true", "false") \
| eval reviewer=coalesce(gen_ai_review_reviewer, "") \
| eval review_completed_at=gen_ai_review_updated_at \
| eval ml_model_version="pii_detection_model" \
| eval extracted_at=now() \
| eval app_name=coalesce('gen_ai.app.name', "Unknown") \
| eval model_name="pii_detection_model" \
| table _key, event_id, trace_id, response_text, response_length, pii_label, pii_types, pii_types_reviewed, has_ssn_label, has_email_label, has_phone_label, has_dob_label, has_address_label, has_credit_card_label, has_name_label, phi_confirmed, reviewer, review_completed_at, ml_predicted_score, ml_predicted_label, ml_predicted_types, ml_model_version, feedback_type, split_assignment, output_length, word_count, digit_ratio, special_char_ratio, uppercase_ratio, has_ssn, has_email, has_phone, has_dob, has_address, has_credit_card, has_name, extracted_at, app_name, model_name \
| outputlookup append=true pii_training_feedback_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 2 * * *
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - PII Feedback Loop - Assign Data Splits]
description = Assigns stratified train/valid/test splits (70/15/15) to feedback data that hasn't been assigned yet. Ensures balanced class distribution across splits.
search = | inputlookup pii_training_feedback_lookup \
| where isnull(split_assignment) OR split_assignment="" \
| eval random_val=random()%100 \
| eval split_assignment=case(random_val<70, "train", random_val<85, "valid", 1=1, "test") \
| table _key, event_id, split_assignment \
| outputlookup append=false pii_training_feedback_lookup key_field=_key
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 30 2 * * *
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - PII Feedback Loop - Train Challenger Model]
description = Weekly training of challenger model using combined original training data and human feedback. Trains LogisticRegression on train split, saves as pii_detection_model_challenger for comparison against champion.
search = | inputlookup pii_training_feedback_lookup \
| where split_assignment="train" \
| rename response_text AS response \
| append \
    [| inputlookup pii_training_data_engineered \
    | eval split_assignment="train" \
    | rename response_text AS response] \
| where isnotnull(pii_label) \
| fit LogisticRegression pii_label \
    from output_length word_count digit_ratio special_char_ratio uppercase_ratio \
    has_ssn has_email has_phone has_dob has_address has_credit_card has_name \
    probabilities=true \
    into app:pii_detection_model_challenger
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 4 * * 0
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - PII Feedback Loop - Threshold Analysis]
description = Analyzes multiple thresholds (0.3-0.8) on validation set to find optimal operating point. Calculates precision, recall, F1 for each threshold and stores results for comparison.
search = | inputlookup pii_training_feedback_lookup \
| where split_assignment="valid" \
| apply pii_detection_model_challenger \
| eval risk_score=round(coalesce('probability(pii_label=1)', 'predicted(pii_label)'), 4) \
| eval pred_03=if(risk_score>0.3, 1, 0) \
| eval pred_04=if(risk_score>0.4, 1, 0) \
| eval pred_05=if(risk_score>0.5, 1, 0) \
| eval pred_06=if(risk_score>0.6, 1, 0) \
| eval pred_07=if(risk_score>0.7, 1, 0) \
| eval pred_08=if(risk_score>0.8, 1, 0) \
| eval tp_03=if(pii_label=1 AND pred_03=1, 1, 0), fp_03=if(pii_label=0 AND pred_03=1, 1, 0), fn_03=if(pii_label=1 AND pred_03=0, 1, 0), tn_03=if(pii_label=0 AND pred_03=0, 1, 0) \
| eval tp_04=if(pii_label=1 AND pred_04=1, 1, 0), fp_04=if(pii_label=0 AND pred_04=1, 1, 0), fn_04=if(pii_label=1 AND pred_04=0, 1, 0), tn_04=if(pii_label=0 AND pred_04=0, 1, 0) \
| eval tp_05=if(pii_label=1 AND pred_05=1, 1, 0), fp_05=if(pii_label=0 AND pred_05=1, 1, 0), fn_05=if(pii_label=1 AND pred_05=0, 1, 0), tn_05=if(pii_label=0 AND pred_05=0, 1, 0) \
| eval tp_06=if(pii_label=1 AND pred_06=1, 1, 0), fp_06=if(pii_label=0 AND pred_06=1, 1, 0), fn_06=if(pii_label=1 AND pred_06=0, 1, 0), tn_06=if(pii_label=0 AND pred_06=0, 1, 0) \
| eval tp_07=if(pii_label=1 AND pred_07=1, 1, 0), fp_07=if(pii_label=0 AND pred_07=1, 1, 0), fn_07=if(pii_label=1 AND pred_07=0, 1, 0), tn_07=if(pii_label=0 AND pred_07=0, 1, 0) \
| eval tp_08=if(pii_label=1 AND pred_08=1, 1, 0), fp_08=if(pii_label=0 AND pred_08=1, 1, 0), fn_08=if(pii_label=1 AND pred_08=0, 1, 0), tn_08=if(pii_label=0 AND pred_08=0, 1, 0) \
| stats sum(tp_03) AS tp_03, sum(fp_03) AS fp_03, sum(fn_03) AS fn_03, sum(tn_03) AS tn_03, \
    sum(tp_04) AS tp_04, sum(fp_04) AS fp_04, sum(fn_04) AS fn_04, sum(tn_04) AS tn_04, \
    sum(tp_05) AS tp_05, sum(fp_05) AS fp_05, sum(fn_05) AS fn_05, sum(tn_05) AS tn_05, \
    sum(tp_06) AS tp_06, sum(fp_06) AS fp_06, sum(fn_06) AS fn_06, sum(tn_06) AS tn_06, \
    sum(tp_07) AS tp_07, sum(fp_07) AS fp_07, sum(fn_07) AS fn_07, sum(tn_07) AS tn_07, \
    sum(tp_08) AS tp_08, sum(fp_08) AS fp_08, sum(fn_08) AS fn_08, sum(tn_08) AS tn_08, \
    count AS total_samples \
| eval precision_03=round(tp_03/(tp_03+fp_03), 4), recall_03=round(tp_03/(tp_03+fn_03), 4), f1_03=round(2*precision_03*recall_03/(precision_03+recall_03), 4) \
| eval precision_04=round(tp_04/(tp_04+fp_04), 4), recall_04=round(tp_04/(tp_04+fn_04), 4), f1_04=round(2*precision_04*recall_04/(precision_04+recall_04), 4) \
| eval precision_05=round(tp_05/(tp_05+fp_05), 4), recall_05=round(tp_05/(tp_05+fn_05), 4), f1_05=round(2*precision_05*recall_05/(precision_05+recall_05), 4) \
| eval precision_06=round(tp_06/(tp_06+fp_06), 4), recall_06=round(tp_06/(tp_06+fn_06), 4), f1_06=round(2*precision_06*recall_06/(precision_06+recall_06), 4) \
| eval precision_07=round(tp_07/(tp_07+fp_07), 4), recall_07=round(tp_07/(tp_07+fn_07), 4), f1_07=round(2*precision_07*recall_07/(precision_07+recall_07), 4) \
| eval precision_08=round(tp_08/(tp_08+fp_08), 4), recall_08=round(tp_08/(tp_08+fn_08), 4), f1_08=round(2*precision_08*recall_08/(precision_08+recall_08), 4) \
| eval best_f1=max(f1_03, f1_04, f1_05, f1_06, f1_07, f1_08) \
| eval optimal_threshold=case(f1_03=best_f1, 0.3, f1_04=best_f1, 0.4, f1_05=best_f1, 0.5, f1_06=best_f1, 0.6, f1_07=best_f1, 0.7, f1_08=best_f1, 0.8) \
| table total_samples, optimal_threshold, \
    precision_03, recall_03, f1_03, tp_03, fp_03, fn_03, tn_03, \
    precision_04, recall_04, f1_04, tp_04, fp_04, fn_04, tn_04, \
    precision_05, recall_05, f1_05, tp_05, fp_05, fn_05, tn_05, \
    precision_06, recall_06, f1_06, tp_06, fp_06, fn_06, tn_06, \
    precision_07, recall_07, f1_07, tp_07, fp_07, fn_07, tn_07, \
    precision_08, recall_08, f1_08, tp_08, fp_08, fn_08, tn_08
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 30 4 * * 0
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - PII Feedback Loop - Champion vs Challenger Report]
description = Compares performance metrics between current champion model and challenger model on test set. Shows accuracy, precision, recall, F1 for both models side-by-side.
search = | inputlookup pii_training_feedback_lookup \
| where split_assignment="test" \
| apply pii_detection_model \
| eval champion_score=round(coalesce('probability(pii_label=1)', 'predicted(pii_label)'), 4) \
| eval champion_pred=if(champion_score>0.5, 1, 0) \
| apply pii_detection_model_challenger \
| eval challenger_score=round(coalesce('probability(pii_label=1)', 'predicted(pii_label)'), 4) \
| eval challenger_pred=if(challenger_score>0.5, 1, 0) \
| eval champion_tp=if(pii_label=1 AND champion_pred=1, 1, 0) \
| eval champion_fp=if(pii_label=0 AND champion_pred=1, 1, 0) \
| eval champion_fn=if(pii_label=1 AND champion_pred=0, 1, 0) \
| eval champion_tn=if(pii_label=0 AND champion_pred=0, 1, 0) \
| eval challenger_tp=if(pii_label=1 AND challenger_pred=1, 1, 0) \
| eval challenger_fp=if(pii_label=0 AND challenger_pred=1, 1, 0) \
| eval challenger_fn=if(pii_label=1 AND challenger_pred=0, 1, 0) \
| eval challenger_tn=if(pii_label=0 AND challenger_pred=0, 1, 0) \
| stats sum(champion_tp) AS champion_tp, sum(champion_fp) AS champion_fp, sum(champion_fn) AS champion_fn, sum(champion_tn) AS champion_tn, \
    sum(challenger_tp) AS challenger_tp, sum(challenger_fp) AS challenger_fp, sum(challenger_fn) AS challenger_fn, sum(challenger_tn) AS challenger_tn, \
    count AS test_samples \
| eval champion_accuracy=round((champion_tp+champion_tn)/test_samples, 4) \
| eval champion_precision=round(champion_tp/(champion_tp+champion_fp), 4) \
| eval champion_recall=round(champion_tp/(champion_tp+champion_fn), 4) \
| eval champion_f1=round(2*champion_precision*champion_recall/(champion_precision+champion_recall), 4) \
| eval challenger_accuracy=round((challenger_tp+challenger_tn)/test_samples, 4) \
| eval challenger_precision=round(challenger_tp/(challenger_tp+challenger_fp), 4) \
| eval challenger_recall=round(challenger_tp/(challenger_tp+challenger_fn), 4) \
| eval challenger_f1=round(2*challenger_precision*challenger_recall/(challenger_precision+challenger_recall), 4) \
| eval accuracy_delta=round(challenger_accuracy-champion_accuracy, 4) \
| eval precision_delta=round(challenger_precision-champion_precision, 4) \
| eval recall_delta=round(challenger_recall-champion_recall, 4) \
| eval f1_delta=round(challenger_f1-champion_f1, 4) \
| eval recommendation=if(challenger_f1>champion_f1 AND challenger_recall>=champion_recall, "PROMOTE CHALLENGER", "KEEP CHAMPION") \
| table test_samples, \
    champion_accuracy, champion_precision, champion_recall, champion_f1, \
    challenger_accuracy, challenger_precision, challenger_recall, challenger_f1, \
    accuracy_delta, precision_delta, recall_delta, f1_delta, \
    recommendation
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 5 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ml-ops@example.com, ai-governance@example.com
action.email.subject = Weekly PII Model Champion vs Challenger Report
action.email.message.alert = Champion F1: $result.champion_f1$\nChallenger F1: $result.challenger_f1$\nDelta: $result.f1_delta$\n\nRecommendation: $result.recommendation$

[GenAI - PII Feedback Loop - Promote Model Version]
description = Manual trigger to promote challenger model to champion. Registers new version in model registry, archives old champion. Run manually after reviewing Champion vs Challenger Report.
search = | makeresults \
| eval model_name="pii_detection_model" \
| eval model_version=strftime(now(), "%Y%m%d_%H%M%S") \
| eval algorithm="LogisticRegression" \
| eval feature_count=20 \
| eval status="champion" \
| eval created_at=now() \
| eval promoted_at=now() \
| eval promoted_by="manual_promotion" \
| eval notes="Promoted from challenger after positive evaluation" \
| join type=left \
    [| inputlookup pii_training_feedback_lookup \
    | where split_assignment="test" \
    | apply pii_detection_model_challenger \
    | eval pred=if(coalesce('probability(pii_label=1)', 'predicted(pii_label)')>0.5, 1, 0) \
    | eval tp=if(pii_label=1 AND pred=1, 1, 0), fp=if(pii_label=0 AND pred=1, 1, 0), fn=if(pii_label=1 AND pred=0, 1, 0), tn=if(pii_label=0 AND pred=0, 1, 0) \
    | stats sum(tp) AS tp, sum(fp) AS fp, sum(fn) AS fn, sum(tn) AS tn, count AS total, sum(pii_label) AS pii_count \
    | eval accuracy=round((tp+tn)/total, 4) \
    | eval precision=round(tp/(tp+fp), 4) \
    | eval recall=round(tp/(tp+fn), 4) \
    | eval f1_score=round(2*precision*recall/(precision+recall), 4) \
    | eval training_samples=total \
    | eval pii_samples=pii_count \
    | eval clean_samples=total-pii_count \
    | eval threshold=0.5 \
    | table accuracy, precision, recall, f1_score, threshold, training_samples, pii_samples, clean_samples] \
| eval _key=model_name."_".model_version \
| table _key, model_name, model_version, algorithm, feature_count, accuracy, precision, recall, f1_score, threshold, training_samples, pii_samples, clean_samples, status, created_at, promoted_at, promoted_by, notes \
| outputlookup append=true pii_model_registry_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
is_visible = true
alert.track = 0

[GenAI - PII Feedback Loop - Model Health Report]
description = Weekly summary of model performance, feedback statistics, and data quality metrics. Monitors for model drift and data distribution changes. Includes per-type label statistics from reviewer-confirmed PII types.
search = | inputlookup pii_training_feedback_lookup \
| stats count AS total_feedback, \
    sum(eval(if(feedback_type="confirmed_pii_exact", 1, 0))) AS confirmed_pii_exact, \
    sum(eval(if(feedback_type="confirmed_pii_type_mismatch", 1, 0))) AS confirmed_pii_type_mismatch, \
    sum(eval(if(feedback_type="confirmed_pii" OR feedback_type="confirmed_pii_exact" OR feedback_type="confirmed_pii_type_mismatch", 1, 0))) AS confirmed_pii, \
    sum(eval(if(feedback_type="false_positive", 1, 0))) AS false_positives, \
    sum(eval(if(feedback_type="false_negative", 1, 0))) AS false_negatives, \
    sum(eval(if(feedback_type="confirmed_clean", 1, 0))) AS confirmed_clean, \
    sum(eval(if(split_assignment="train", 1, 0))) AS train_count, \
    sum(eval(if(split_assignment="valid", 1, 0))) AS valid_count, \
    sum(eval(if(split_assignment="test", 1, 0))) AS test_count, \
    sum(has_ssn_label) AS ssn_confirmed, \
    sum(has_email_label) AS email_confirmed, \
    sum(has_phone_label) AS phone_confirmed, \
    sum(has_name_label) AS name_confirmed, \
    sum(has_dob_label) AS dob_confirmed, \
    sum(has_address_label) AS address_confirmed, \
    sum(has_credit_card_label) AS credit_card_confirmed, \
    avg(ml_predicted_score) AS avg_ml_score, \
    dc(app_name) AS unique_apps, \
    dc(reviewer) AS unique_reviewers, \
    max(extracted_at) AS last_extraction \
| eval false_positive_rate=round(false_positives/total_feedback*100, 2) \
| eval false_negative_rate=round(false_negatives/total_feedback*100, 2) \
| eval type_mismatch_rate=if(confirmed_pii>0, round(confirmed_pii_type_mismatch/confirmed_pii*100, 2), 0) \
| eval model_health=case( \
    false_negative_rate > 20, "CRITICAL - High false negative rate", \
    false_positive_rate > 30, "WARNING - High false positive rate", \
    type_mismatch_rate > 50, "WARNING - High type mismatch rate", \
    total_feedback < 50, "WARNING - Insufficient feedback data", \
    1=1, "HEALTHY" \
) \
| eval last_extraction_time=strftime(last_extraction, "%Y-%m-%d %H:%M:%S") \
| table total_feedback, confirmed_pii, confirmed_pii_exact, confirmed_pii_type_mismatch, confirmed_clean, false_positives, false_negatives, \
    false_positive_rate, false_negative_rate, type_mismatch_rate, \
    ssn_confirmed, email_confirmed, phone_confirmed, name_confirmed, dob_confirmed, address_confirmed, credit_card_confirmed, \
    train_count, valid_count, test_count, \
    avg_ml_score, unique_apps, unique_reviewers, \
    last_extraction_time, model_health
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 9 * * 1
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = ml-ops@example.com, ai-governance@example.com
action.email.subject = Weekly PII Feedback Loop Model Health Report
action.email.message.alert = Model Health: $result.model_health$\n\nTotal Feedback: $result.total_feedback$\nFalse Positive Rate: $result.false_positive_rate$%\nFalse Negative Rate: $result.false_negative_rate$%\n\nTrain/Valid/Test Split: $result.train_count$/$result.valid_count$/$result.test_count$

###############################################################################
# PROMPT INJECTION FEEDBACK LOOP - ACTIVE LEARNING SEARCHES
# These searches implement the active learning feedback loop for prompt injection detection
###############################################################################

[GenAI - Prompt Injection Feedback Loop - Extract Training Feedback]
description = Daily extraction of human-labeled reviews into training feedback collection. Uses reviewer-confirmed injection techniques as ground truth labels. Classifies feedback type by comparing ML predictions with reviewer confirmations, assigns train/valid/test splits, and stores for model retraining.
search = | inputlookup gen_ai_review_findings_lookup \
| search gen_ai_review_status="completed" \
| eval reviewed_technique=coalesce(gen_ai_review_prompt_injection_type, "") \
| eval injection_label=if(gen_ai_review_prompt_injection_confirmed="true" OR gen_ai_review_prompt_injection_confirmed="1", 1, 0) \
| eval has_ignore_instruction_label=if(match(reviewed_technique, "(?i)ignore_instructions"), 1, 0) \
| eval has_reveal_request_label=if(match(reviewed_technique, "(?i)reveal_system"), 1, 0) \
| eval has_bypass_request_label=if(match(reviewed_technique, "(?i)bypass_safety"), 1, 0) \
| eval has_roleplay_injection_label=if(match(reviewed_technique, "(?i)roleplay_injection"), 1, 0) \
| eval has_jailbreak_terms_label=if(match(reviewed_technique, "(?i)jailbreak"), 1, 0) \
| eval has_encoding_label=if(match(reviewed_technique, "(?i)encoding_obfuscation"), 1, 0) \
| join type=left gen_ai_event_id \
    [search index=gen_ai_log gen_ai.input.messages=* earliest=-90d \
    | dedup gen_ai.event.id \
    | rename gen_ai.event.id AS gen_ai_event_id \
    | eval input_text=coalesce('gen_ai.input.messages', 'event.input', input) \
    | table gen_ai_event_id, input_text, gen_ai.prompt_injection.risk_score, gen_ai.prompt_injection.ml_detected, gen_ai.prompt_injection.technique, gen_ai.app.name, gen_ai.trace.id] \
| where isnotnull(input_text) AND len(input_text) > 10 \
| lookup prompt_injection_training_feedback_lookup event_id AS gen_ai_event_id OUTPUT event_id AS existing_event \
| where isnull(existing_event) \
| eval ml_predicted_score=coalesce('gen_ai.prompt_injection.risk_score', 0) \
| eval ml_predicted_label=coalesce('gen_ai.prompt_injection.ml_detected', "false") \
| eval ml_predicted_technique=coalesce('gen_ai.prompt_injection.technique', "") \
| eval feedback_type=case( \
    injection_label=1 AND ml_predicted_label="true" AND reviewed_technique=ml_predicted_technique, "confirmed_attack_exact", \
    injection_label=1 AND ml_predicted_label="true", "confirmed_attack_technique_mismatch", \
    injection_label=1 AND ml_predicted_label="false", "false_negative", \
    injection_label=0 AND ml_predicted_label="true", "false_positive", \
    injection_label=0 AND ml_predicted_label="false", "confirmed_clean" \
) \
| eval random_val=random()%100 \
| eval split_assignment=case(random_val<70, "train", random_val<85, "valid", 1=1, "test") \
| eval prompt_length=len(input_text) \
| eval word_count=mvcount(split(input_text, " ")) \
| eval has_ignore_instruction=if(match(input_text, "(?i)(ignore|disregard|forget).*(previous|prior|above|earlier).*(instruction|prompt|rule|guideline)"), 1, 0) \
| eval has_reveal_request=if(match(input_text, "(?i)(reveal|show|display|tell me|what is).*(system|original|hidden).*(prompt|instruction|rule)"), 1, 0) \
| eval has_bypass_request=if(match(input_text, "(?i)(bypass|override|disable|turn off).*(safety|filter|guardrail|restriction|limit)"), 1, 0) \
| eval has_roleplay_injection=if(match(input_text, "(?i)(pretend|act as|imagine you are|you are now|from now on).*(unrestricted|unfiltered|without limits|no rules)"), 1, 0) \
| eval has_jailbreak_terms=if(match(input_text, "(?i)(jailbreak|DAN mode|sudo mode|developer mode|god mode|admin mode|STAN|DUDE|AntiGPT)"), 1, 0) \
| eval has_encoding=if(match(input_text, "(\\\\x[0-9a-fA-F]{2}|%[0-9a-fA-F]{2}|&#\d+;|&#x[0-9a-fA-F]+;|base64|rot13|decode)"), 1, 0) \
| eval special_char_count=len(replace(input_text, "[A-Za-z0-9\s]", "")) \
| eval special_char_ratio=if(prompt_length>0, round(special_char_count/prompt_length, 4), 0) \
| rex field=input_text max_match=100 "(?i)(?<negation_match>don't|do not|never|not|no|none)" \
| eval negation_count=if(isnull(negation_match), 0, mvcount(negation_match)) \
| eval negation_density=if(word_count>0, round(negation_count/word_count, 4), 0) \
| eval starts_with_command=if(match(input_text, "(?i)^(ignore|disregard|forget|reveal|show|tell|bypass|override|enable|activate|switch|enter|turn)"), 1, 0) \
| eval _key=gen_ai_event_id \
| eval event_id=gen_ai_event_id \
| eval trace_id=coalesce('gen_ai.trace.id', "") \
| eval injection_technique=ml_predicted_technique \
| eval technique_reviewed=reviewed_technique \
| eval reviewer=coalesce(gen_ai_review_reviewer, "") \
| eval review_completed_at=gen_ai_review_updated_at \
| eval ml_model_version="prompt_injection_model" \
| eval extracted_at=now() \
| eval app_name=coalesce('gen_ai.app.name', "Unknown") \
| eval model_name="prompt_injection_model" \
| table _key, event_id, trace_id, input_text, prompt_length, injection_label, injection_technique, technique_reviewed, has_ignore_instruction_label, has_reveal_request_label, has_bypass_request_label, has_roleplay_injection_label, has_jailbreak_terms_label, has_encoding_label, reviewer, review_completed_at, ml_predicted_score, ml_predicted_label, ml_predicted_technique, ml_model_version, feedback_type, split_assignment, word_count, special_char_ratio, negation_density, has_ignore_instruction, has_reveal_request, has_bypass_request, has_roleplay_injection, has_jailbreak_terms, has_encoding, starts_with_command, extracted_at, app_name, model_name \
| outputlookup append=true prompt_injection_training_feedback_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 2 * * *
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - Prompt Injection Feedback Loop - Train Challenger Model]
description = Weekly training of challenger model using combined original training data and human feedback. Trains RandomForestClassifier on train split, saves as prompt_injection_model_challenger for comparison against champion.
search = | inputlookup prompt_injection_training_feedback_lookup \
| where split_assignment="train" \
| append \
    [| inputlookup prompt_injection_training_data_engineered \
    | eval split_assignment="train"] \
| where isnotnull(injection_label) \
| fit RandomForestClassifier injection_label \
    from prompt_length word_count special_char_ratio negation_density \
    has_ignore_instruction has_reveal_request has_bypass_request \
    has_roleplay_injection has_jailbreak_terms has_encoding starts_with_command \
    max_depth=15 \
    max_features=8 \
    n_estimators=100 \
    random_state=42 \
    into app:prompt_injection_model_challenger
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 4 * * 0
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - Prompt Injection Feedback Loop - Threshold Analysis]
description = Analyzes multiple thresholds (0.4-0.9) on validation set to find optimal operating point for prompt injection detection. Higher thresholds recommended due to security sensitivity.
search = | inputlookup prompt_injection_training_feedback_lookup \
| where split_assignment="valid" \
| apply prompt_injection_model_challenger \
| eval risk_score=round('predicted(injection_label)', 4) \
| eval pred_04=if(risk_score>0.4, 1, 0) \
| eval pred_05=if(risk_score>0.5, 1, 0) \
| eval pred_06=if(risk_score>0.6, 1, 0) \
| eval pred_07=if(risk_score>0.7, 1, 0) \
| eval pred_08=if(risk_score>0.8, 1, 0) \
| eval pred_09=if(risk_score>0.9, 1, 0) \
| eval tp_04=if(injection_label=1 AND pred_04=1, 1, 0), fp_04=if(injection_label=0 AND pred_04=1, 1, 0), fn_04=if(injection_label=1 AND pred_04=0, 1, 0), tn_04=if(injection_label=0 AND pred_04=0, 1, 0) \
| eval tp_05=if(injection_label=1 AND pred_05=1, 1, 0), fp_05=if(injection_label=0 AND pred_05=1, 1, 0), fn_05=if(injection_label=1 AND pred_05=0, 1, 0), tn_05=if(injection_label=0 AND pred_05=0, 1, 0) \
| eval tp_06=if(injection_label=1 AND pred_06=1, 1, 0), fp_06=if(injection_label=0 AND pred_06=1, 1, 0), fn_06=if(injection_label=1 AND pred_06=0, 1, 0), tn_06=if(injection_label=0 AND pred_06=0, 1, 0) \
| eval tp_07=if(injection_label=1 AND pred_07=1, 1, 0), fp_07=if(injection_label=0 AND pred_07=1, 1, 0), fn_07=if(injection_label=1 AND pred_07=0, 1, 0), tn_07=if(injection_label=0 AND pred_07=0, 1, 0) \
| eval tp_08=if(injection_label=1 AND pred_08=1, 1, 0), fp_08=if(injection_label=0 AND pred_08=1, 1, 0), fn_08=if(injection_label=1 AND pred_08=0, 1, 0), tn_08=if(injection_label=0 AND pred_08=0, 1, 0) \
| eval tp_09=if(injection_label=1 AND pred_09=1, 1, 0), fp_09=if(injection_label=0 AND pred_09=1, 1, 0), fn_09=if(injection_label=1 AND pred_09=0, 1, 0), tn_09=if(injection_label=0 AND pred_09=0, 1, 0) \
| stats sum(tp_04) AS tp_04, sum(fp_04) AS fp_04, sum(fn_04) AS fn_04, sum(tn_04) AS tn_04, \
    sum(tp_05) AS tp_05, sum(fp_05) AS fp_05, sum(fn_05) AS fn_05, sum(tn_05) AS tn_05, \
    sum(tp_06) AS tp_06, sum(fp_06) AS fp_06, sum(fn_06) AS fn_06, sum(tn_06) AS tn_06, \
    sum(tp_07) AS tp_07, sum(fp_07) AS fp_07, sum(fn_07) AS fn_07, sum(tn_07) AS tn_07, \
    sum(tp_08) AS tp_08, sum(fp_08) AS fp_08, sum(fn_08) AS fn_08, sum(tn_08) AS tn_08, \
    sum(tp_09) AS tp_09, sum(fp_09) AS fp_09, sum(fn_09) AS fn_09, sum(tn_09) AS tn_09, \
    count AS total_samples \
| eval precision_04=round(tp_04/(tp_04+fp_04), 4), recall_04=round(tp_04/(tp_04+fn_04), 4), f1_04=round(2*precision_04*recall_04/(precision_04+recall_04), 4) \
| eval precision_05=round(tp_05/(tp_05+fp_05), 4), recall_05=round(tp_05/(tp_05+fn_05), 4), f1_05=round(2*precision_05*recall_05/(precision_05+recall_05), 4) \
| eval precision_06=round(tp_06/(tp_06+fp_06), 4), recall_06=round(tp_06/(tp_06+fn_06), 4), f1_06=round(2*precision_06*recall_06/(precision_06+recall_06), 4) \
| eval precision_07=round(tp_07/(tp_07+fp_07), 4), recall_07=round(tp_07/(tp_07+fn_07), 4), f1_07=round(2*precision_07*recall_07/(precision_07+recall_07), 4) \
| eval precision_08=round(tp_08/(tp_08+fp_08), 4), recall_08=round(tp_08/(tp_08+fn_08), 4), f1_08=round(2*precision_08*recall_08/(precision_08+recall_08), 4) \
| eval precision_09=round(tp_09/(tp_09+fp_09), 4), recall_09=round(tp_09/(tp_09+fn_09), 4), f1_09=round(2*precision_09*recall_09/(precision_09+recall_09), 4) \
| eval best_f1=max(f1_04, f1_05, f1_06, f1_07, f1_08, f1_09) \
| eval optimal_threshold=case(f1_04=best_f1, 0.4, f1_05=best_f1, 0.5, f1_06=best_f1, 0.6, f1_07=best_f1, 0.7, f1_08=best_f1, 0.8, f1_09=best_f1, 0.9) \
| table total_samples, optimal_threshold, \
    precision_04, recall_04, f1_04, tp_04, fp_04, fn_04, tn_04, \
    precision_05, recall_05, f1_05, tp_05, fp_05, fn_05, tn_05, \
    precision_06, recall_06, f1_06, tp_06, fp_06, fn_06, tn_06, \
    precision_07, recall_07, f1_07, tp_07, fp_07, fn_07, tn_07, \
    precision_08, recall_08, f1_08, tp_08, fp_08, fn_08, tn_08, \
    precision_09, recall_09, f1_09, tp_09, fp_09, fn_09, tn_09
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 30 4 * * 0
enableSched = 1
is_visible = true
alert.track = 0

[GenAI - Prompt Injection Feedback Loop - Champion vs Challenger Report]
description = Compares performance metrics between current champion model and challenger model on test set for prompt injection detection. Shows accuracy, precision, recall, F1 for both models side-by-side.
search = | inputlookup prompt_injection_training_feedback_lookup \
| where split_assignment="test" \
| apply prompt_injection_model \
| eval champion_score=round('predicted(injection_label)', 4) \
| eval champion_pred=if(champion_score>0.6, 1, 0) \
| apply prompt_injection_model_challenger \
| eval challenger_score=round('predicted(injection_label)', 4) \
| eval challenger_pred=if(challenger_score>0.6, 1, 0) \
| eval champion_tp=if(injection_label=1 AND champion_pred=1, 1, 0) \
| eval champion_fp=if(injection_label=0 AND champion_pred=1, 1, 0) \
| eval champion_fn=if(injection_label=1 AND champion_pred=0, 1, 0) \
| eval champion_tn=if(injection_label=0 AND champion_pred=0, 1, 0) \
| eval challenger_tp=if(injection_label=1 AND challenger_pred=1, 1, 0) \
| eval challenger_fp=if(injection_label=0 AND challenger_pred=1, 1, 0) \
| eval challenger_fn=if(injection_label=1 AND challenger_pred=0, 1, 0) \
| eval challenger_tn=if(injection_label=0 AND challenger_pred=0, 1, 0) \
| stats sum(champion_tp) AS champion_tp, sum(champion_fp) AS champion_fp, sum(champion_fn) AS champion_fn, sum(champion_tn) AS champion_tn, \
    sum(challenger_tp) AS challenger_tp, sum(challenger_fp) AS challenger_fp, sum(challenger_fn) AS challenger_fn, sum(challenger_tn) AS challenger_tn, \
    count AS test_samples \
| eval champion_accuracy=round((champion_tp+champion_tn)/test_samples, 4) \
| eval champion_precision=round(champion_tp/(champion_tp+champion_fp), 4) \
| eval champion_recall=round(champion_tp/(champion_tp+champion_fn), 4) \
| eval champion_f1=round(2*champion_precision*champion_recall/(champion_precision+champion_recall), 4) \
| eval challenger_accuracy=round((challenger_tp+challenger_tn)/test_samples, 4) \
| eval challenger_precision=round(challenger_tp/(challenger_tp+challenger_fp), 4) \
| eval challenger_recall=round(challenger_tp/(challenger_tp+challenger_fn), 4) \
| eval challenger_f1=round(2*challenger_precision*challenger_recall/(challenger_precision+challenger_recall), 4) \
| eval accuracy_delta=round(challenger_accuracy-champion_accuracy, 4) \
| eval precision_delta=round(challenger_precision-champion_precision, 4) \
| eval recall_delta=round(challenger_recall-champion_recall, 4) \
| eval f1_delta=round(challenger_f1-champion_f1, 4) \
| eval recommendation=if(challenger_f1>champion_f1 AND challenger_recall>=champion_recall, "PROMOTE CHALLENGER", "KEEP CHAMPION") \
| table test_samples, \
    champion_accuracy, champion_precision, champion_recall, champion_f1, \
    challenger_accuracy, challenger_precision, challenger_recall, challenger_f1, \
    accuracy_delta, precision_delta, recall_delta, f1_delta, \
    recommendation
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 5 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = Weekly Prompt Injection Model Champion vs Challenger Report
action.email.message.alert = Champion F1: $result.champion_f1$\nChallenger F1: $result.challenger_f1$\nDelta: $result.f1_delta$\n\nRecommendation: $result.recommendation$

[GenAI - Prompt Injection Feedback Loop - Promote Model Version]
description = Manual trigger to promote challenger model to champion for prompt injection detection. Registers new version in model registry, archives old champion. Run manually after reviewing Champion vs Challenger Report.
search = | makeresults \
| eval model_name="prompt_injection_model" \
| eval model_version=strftime(now(), "%Y%m%d_%H%M%S") \
| eval algorithm="RandomForestClassifier" \
| eval feature_count=11 \
| eval status="champion" \
| eval created_at=now() \
| eval promoted_at=now() \
| eval promoted_by="manual_promotion" \
| eval notes="Promoted from challenger after positive evaluation" \
| join type=left \
    [| inputlookup prompt_injection_training_feedback_lookup \
    | where split_assignment="test" \
    | apply prompt_injection_model_challenger \
    | eval pred=if('predicted(injection_label)'>0.6, 1, 0) \
    | eval tp=if(injection_label=1 AND pred=1, 1, 0), fp=if(injection_label=0 AND pred=1, 1, 0), fn=if(injection_label=1 AND pred=0, 1, 0), tn=if(injection_label=0 AND pred=0, 1, 0) \
    | stats sum(tp) AS tp, sum(fp) AS fp, sum(fn) AS fn, sum(tn) AS tn, count AS total, sum(injection_label) AS attack_count \
    | eval accuracy=round((tp+tn)/total, 4) \
    | eval precision=round(tp/(tp+fp), 4) \
    | eval recall=round(tp/(tp+fn), 4) \
    | eval f1_score=round(2*precision*recall/(precision+recall), 4) \
    | eval training_samples=total \
    | eval attack_samples=attack_count \
    | eval clean_samples=total-attack_count \
    | eval threshold=0.6 \
    | table accuracy, precision, recall, f1_score, threshold, training_samples, attack_samples, clean_samples] \
| eval _key=model_name."_".model_version \
| table _key, model_name, model_version, algorithm, feature_count, accuracy, precision, recall, f1_score, threshold, training_samples, attack_samples, clean_samples, status, created_at, promoted_at, promoted_by, notes \
| outputlookup append=true prompt_injection_model_registry_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
is_visible = true
alert.track = 0

[GenAI - Prompt Injection Feedback Loop - Model Health Report]
description = Weekly summary of prompt injection model performance, feedback statistics, and data quality metrics. Monitors for model drift and data distribution changes. Includes per-technique label statistics from reviewer-confirmed attack types.
search = | inputlookup prompt_injection_training_feedback_lookup \
| stats count AS total_feedback, \
    sum(eval(if(feedback_type="confirmed_attack_exact", 1, 0))) AS confirmed_attack_exact, \
    sum(eval(if(feedback_type="confirmed_attack_technique_mismatch", 1, 0))) AS confirmed_attack_technique_mismatch, \
    sum(eval(if(feedback_type="confirmed_attack_exact" OR feedback_type="confirmed_attack_technique_mismatch", 1, 0))) AS confirmed_attacks, \
    sum(eval(if(feedback_type="false_positive", 1, 0))) AS false_positives, \
    sum(eval(if(feedback_type="false_negative", 1, 0))) AS false_negatives, \
    sum(eval(if(feedback_type="confirmed_clean", 1, 0))) AS confirmed_clean, \
    sum(eval(if(split_assignment="train", 1, 0))) AS train_count, \
    sum(eval(if(split_assignment="valid", 1, 0))) AS valid_count, \
    sum(eval(if(split_assignment="test", 1, 0))) AS test_count, \
    sum(has_ignore_instruction_label) AS ignore_instruction_confirmed, \
    sum(has_reveal_request_label) AS reveal_request_confirmed, \
    sum(has_bypass_request_label) AS bypass_request_confirmed, \
    sum(has_roleplay_injection_label) AS roleplay_injection_confirmed, \
    sum(has_jailbreak_terms_label) AS jailbreak_confirmed, \
    sum(has_encoding_label) AS encoding_confirmed, \
    avg(ml_predicted_score) AS avg_ml_score, \
    dc(app_name) AS unique_apps, \
    dc(reviewer) AS unique_reviewers, \
    max(extracted_at) AS last_extraction \
| eval false_positive_rate=round(false_positives/total_feedback*100, 2) \
| eval false_negative_rate=round(false_negatives/total_feedback*100, 2) \
| eval technique_mismatch_rate=if(confirmed_attacks>0, round(confirmed_attack_technique_mismatch/confirmed_attacks*100, 2), 0) \
| eval model_health=case( \
    false_negative_rate > 15, "CRITICAL - High false negative rate (security risk)", \
    false_positive_rate > 25, "WARNING - High false positive rate", \
    technique_mismatch_rate > 50, "WARNING - High technique mismatch rate", \
    total_feedback < 30, "WARNING - Insufficient feedback data", \
    1=1, "HEALTHY" \
) \
| eval last_extraction_time=strftime(last_extraction, "%Y-%m-%d %H:%M:%S") \
| table total_feedback, confirmed_attacks, confirmed_attack_exact, confirmed_attack_technique_mismatch, confirmed_clean, false_positives, false_negatives, \
    false_positive_rate, false_negative_rate, technique_mismatch_rate, \
    ignore_instruction_confirmed, reveal_request_confirmed, bypass_request_confirmed, roleplay_injection_confirmed, jailbreak_confirmed, encoding_confirmed, \
    train_count, valid_count, test_count, \
    avg_ml_score, unique_apps, unique_reviewers, \
    last_extraction_time, model_health
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 9 * * 1
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = Weekly Prompt Injection Feedback Loop Model Health Report
action.email.message.alert = Model Health: $result.model_health$\n\nTotal Feedback: $result.total_feedback$\nFalse Positive Rate: $result.false_positive_rate$%\nFalse Negative Rate: $result.false_negative_rate$%\n\nTrain/Valid/Test Split: $result.train_count$/$result.valid_count$/$result.test_count$

###############################################################################
# TF-IDF ANOMALY FEEDBACK LOOP SAVED SEARCHES
# Active learning pipeline for continuous improvement of TF-IDF anomaly models
# 
# Feedback Logic:
# - When reviewer marks "Prompt Anomaly (Reviewed) = FALSE"  True Negative
#    Add prompt text to normal training data
# - When reviewer marks "Response Anomaly (Reviewed) = FALSE"  True Negative
#    Add response text to normal training data
# - When reviewer marks TRUE or n/a  IGNORE (not used for feedback)
###############################################################################

[GenAI - TFIDF Feedback Loop - Extract Training Feedback]
description = Daily extraction of human-confirmed normal examples (TRUE NEGATIVES) from Event Review into training feedback collection. Only extracts records where reviewer marked Prompt Anomaly (Reviewed) = FALSE or Response Anomaly (Reviewed) = FALSE. TRUE and n/a reviews are IGNORED.
search = | inputlookup gen_ai_review_findings_lookup \
| search gen_ai_review_status="completed" \
| eval prompt_reviewed=coalesce(gen_ai_review_anomaly_prompt_reviewed, "n/a") \
| eval response_reviewed=coalesce(gen_ai_review_anomaly_response_reviewed, "n/a") \
| where prompt_reviewed="FALSE" OR response_reviewed="FALSE" \
| eval has_prompt_feedback=if(prompt_reviewed="FALSE", 1, 0) \
| eval has_response_feedback=if(response_reviewed="FALSE", 1, 0) \
| join type=left gen_ai_event_id \
    [search index=gen_ai_log earliest=-90d \
        [| inputlookup gen_ai_review_findings_lookup \
        | search gen_ai_review_status="completed" \
        | eval prompt_reviewed=coalesce(gen_ai_review_anomaly_prompt_reviewed, "n/a") \
        | eval response_reviewed=coalesce(gen_ai_review_anomaly_response_reviewed, "n/a") \
        | where prompt_reviewed="FALSE" OR response_reviewed="FALSE" \
        | rename gen_ai_event_id AS gen_ai.event.id \
        | fields gen_ai.event.id \
        | format] \
    | stats first(gen_ai.input.messages) AS prompt_text, first(gen_ai.output.messages) AS response_text, first(gen_ai.app.name) AS gen_ai.app.name, first(gen_ai.request.model) AS gen_ai.request.model, first(gen_ai.trace.id) AS gen_ai.trace.id by gen_ai.event.id \
    | rename gen_ai.event.id AS gen_ai_event_id] \
| where (has_prompt_feedback=1 AND len(prompt_text)>10) OR (has_response_feedback=1 AND len(response_text)>20) \
| lookup tfidf_training_feedback_lookup gen_ai_event_id OUTPUT gen_ai_event_id AS existing_event \
| where isnull(existing_event) \
| eval feedback_timestamp=now() \
| eval random_val=random()%100 \
| eval split_assignment=case(random_val<70, "train", random_val<85, "valid", 1=1, "test") \
| eval reviewer=coalesce(gen_ai_review_updated_by, gen_ai_review_created_by, "unknown") \
| eval review_completed_at=coalesce(gen_ai_review_updated_at, gen_ai_review_created_at) \
| eval app_name='gen_ai.app.name' \
| eval model_name='gen_ai.request.model' \
| eval trace_id='gen_ai.trace.id' \
| table gen_ai_event_id, trace_id, feedback_timestamp, prompt_text, response_text, has_prompt_feedback, has_response_feedback, app_name, model_name, split_assignment, reviewer, review_completed_at \
| outputlookup append=true tfidf_training_feedback_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = */5 * * * *
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 3

[GenAI - TFIDF Feedback Loop - Step 1 - Train Prompt Challenger PCA]
description = Step 1 of 5: Weekly training of challenger PCA model for prompts. Combines original training data from tfidf_training_data_v3 with human-confirmed normal prompts from feedback. Run this FIRST before training the anomaly model.
search = | inputlookup tfidf_training_data_v3 \
| rename prompt AS input_text \
| where isnotnull(input_text) AND len(input_text) > 10 \
| eval source="original" \
| append \
    [| inputlookup tfidf_training_feedback_lookup \
    | where has_prompt_feedback=1 AND split_assignment="train" \
    | rename prompt_text AS input_text \
    | where isnotnull(input_text) AND len(input_text) > 10 \
    | eval source="feedback"] \
| eval input_text_clean=lower(input_text) \
| eval input_text_clean=replace(input_text_clean, "[^a-z0-9\s]", " ") \
| eval input_text_clean=replace(input_text_clean, "\s+", " ") \
| eval input_text_clean=trim(input_text_clean) \
| where len(input_text_clean) > 20 \
| fit HashingVectorizer input_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| fit PCA "input_text_clean_hashed_*" k=50 into app:tfidf_prompt_pca_challenger
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 15 4 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 3

[GenAI - TFIDF Feedback Loop - Step 2 - Train Prompt Challenger Model]
description = Step 2 of 5: Weekly training of challenger OneClassSVM anomaly model for prompts. Run AFTER Step 1 (PCA training) completes. Combines original data with feedback for improved normal boundary detection.
search = | inputlookup tfidf_training_data_v3 \
| rename prompt AS input_text \
| where isnotnull(input_text) AND len(input_text) > 10 \
| eval source="original" \
| append \
    [| inputlookup tfidf_training_feedback_lookup \
    | where has_prompt_feedback=1 AND split_assignment="train" \
    | rename prompt_text AS input_text \
    | where isnotnull(input_text) AND len(input_text) > 10 \
    | eval source="feedback"] \
| eval input_text_clean=lower(input_text) \
| eval input_text_clean=replace(input_text_clean, "[^a-z0-9\s]", " ") \
| eval input_text_clean=replace(input_text_clean, "\s+", " ") \
| eval input_text_clean=trim(input_text_clean) \
| where len(input_text_clean) > 20 \
| fit HashingVectorizer input_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| apply app:tfidf_prompt_pca_challenger \
| fit OneClassSVM "PC_*" kernel=rbf nu=0.25 into app:prompt_anomaly_model_challenger
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 30 4 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 3

[GenAI - TFIDF Feedback Loop - Step 3 - Train Response Challenger PCA]
description = Step 3 of 5: Weekly training of challenger PCA model for responses. Combines original training data with human-confirmed normal responses from feedback. Run this FIRST before training the response anomaly model.
search = | inputlookup tfidf_training_data_v3 \
| rename response AS output_text \
| where isnotnull(output_text) AND len(output_text) > 20 \
| eval source="original" \
| append \
    [| inputlookup tfidf_training_feedback_lookup \
    | where has_response_feedback=1 AND split_assignment="train" \
    | rename response_text AS output_text \
    | where isnotnull(output_text) AND len(output_text) > 20 \
    | eval source="feedback"] \
| eval output_text_clean=lower(output_text) \
| eval output_text_clean=replace(output_text_clean, "[^a-z0-9\s]", " ") \
| eval output_text_clean=replace(output_text_clean, "\s+", " ") \
| eval output_text_clean=trim(output_text_clean) \
| where len(output_text_clean) > 50 \
| fit HashingVectorizer output_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| fit PCA "output_text_clean_hashed_*" k=20 into app:tfidf_response_pca_challenger
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 45 4 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 3

[GenAI - TFIDF Feedback Loop - Step 4 - Train Response Challenger Model]
description = Step 4 of 5: Weekly training of challenger OneClassSVM anomaly model for responses. Run AFTER Step 3 (PCA training) completes. Combines original data with feedback for improved normal boundary detection.
search = | inputlookup tfidf_training_data_v3 \
| rename response AS output_text \
| where isnotnull(output_text) AND len(output_text) > 20 \
| eval source="original" \
| append \
    [| inputlookup tfidf_training_feedback_lookup \
    | where has_response_feedback=1 AND split_assignment="train" \
    | rename response_text AS output_text \
    | where isnotnull(output_text) AND len(output_text) > 20 \
    | eval source="feedback"] \
| eval output_text_clean=lower(output_text) \
| eval output_text_clean=replace(output_text_clean, "[^a-z0-9\s]", " ") \
| eval output_text_clean=replace(output_text_clean, "\s+", " ") \
| eval output_text_clean=trim(output_text_clean) \
| where len(output_text_clean) > 50 \
| fit HashingVectorizer output_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| apply app:tfidf_response_pca_challenger \
| fit OneClassSVM "PC_*" kernel=rbf nu=0.25 into app:response_anomaly_model_challenger
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 0 5 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 3

[GenAI - TFIDF Feedback Loop - Step 5 - Threshold Analysis]
description = Step 5 of 5: Weekly analysis of anomaly score thresholds for challenger models. Evaluates different thresholds on validation set to find optimal cutoff. Uses OneClassSVM predicted distance for scoring. Run after Steps 1-4 complete.
search = | inputlookup tfidf_training_feedback_lookup \
| where has_prompt_feedback=1 AND split_assignment="valid" \
| rename prompt_text AS input_text \
| where isnotnull(input_text) AND len(input_text) > 10 \
| eval input_text_clean=lower(input_text) \
| eval input_text_clean=replace(input_text_clean, "[^a-z0-9\s]", " ") \
| eval input_text_clean=replace(input_text_clean, "\s+", " ") \
| eval input_text_clean=trim(input_text_clean) \
| where len(input_text_clean) > 20 \
| fit HashingVectorizer input_text_clean max_features=1000 ngram_range=1-2 stop_words=english reduce=false \
| apply app:tfidf_prompt_pca_challenger \
| apply app:prompt_anomaly_model_challenger \
| eval anomaly_score='predicted(score)' \
| stats count AS total_samples, \
    avg(anomaly_score) AS avg_score, \
    stdev(anomaly_score) AS std_score, \
    min(anomaly_score) AS min_score, \
    max(anomaly_score) AS max_score, \
    perc10(anomaly_score) AS p10_score, \
    perc50(anomaly_score) AS p50_score, \
    perc90(anomaly_score) AS p90_score \
| eval model_name="prompt_anomaly_model_challenger" \
| eval model_type="prompt" \
| eval model_version=strftime(now(), "%Y%m%d") \
| eval analysis_date=now() \
| eval threshold=0.5 \
| eval anomaly_count=0 \
| eval normal_count=total_samples \
| eval anomaly_rate=0 \
| eval is_optimal=1 \
| eval notes="Validation set contains only human-confirmed normal prompts" \
| table model_name, model_type, model_version, analysis_date, threshold, total_samples, anomaly_count, normal_count, anomaly_rate, avg_score, std_score, min_score, max_score, p10_score, p50_score, p90_score, is_optimal, notes \
| outputlookup append=true tfidf_threshold_results_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 15 5 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 3

[GenAI - TFIDF Feedback Loop - Champion vs Challenger Report]
description = Weekly comparison of champion and challenger model performance on validation data. Since OneClassSVM only has normal data, comparison is based on score distribution (challenger should have tighter distribution on feedback-augmented data).
search = | makeresults \
| eval model_type="prompt" \
| append [| makeresults | eval model_type="response"] \
| mvexpand model_type \
| join type=left model_type \
    [| inputlookup tfidf_training_feedback_lookup \
    | where split_assignment="valid" \
    | eval has_feedback=if(has_prompt_feedback=1 OR has_response_feedback=1, 1, 0) \
    | stats count AS validation_samples, sum(has_prompt_feedback) AS prompt_samples, sum(has_response_feedback) AS response_samples] \
| eval champion_model=if(model_type="prompt", "prompt_anomaly_model", "response_anomaly_model") \
| eval challenger_model=if(model_type="prompt", "prompt_anomaly_model_challenger", "response_anomaly_model_challenger") \
| eval champion_pca=if(model_type="prompt", "tfidf_prompt_pca", "tfidf_response_pca") \
| eval challenger_pca=if(model_type="prompt", "tfidf_prompt_pca_challenger", "tfidf_response_pca_challenger") \
| eval sample_count=if(model_type="prompt", prompt_samples, response_samples) \
| eval recommendation=case( \
    sample_count < 50, "INSUFFICIENT_DATA - Need more feedback samples", \
    1=1, "MANUAL_REVIEW - Compare score distributions on Model Comparison dashboard" \
) \
| eval report_date=now() \
| table model_type, champion_model, challenger_model, sample_count, recommendation, report_date
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 15 5 * * 0
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = Weekly TF-IDF Anomaly Model Champion vs Challenger Report
action.email.message.alert = Model Type: $result.model_type$\nFeedback Sample Count: $result.sample_count$\nRecommendation: $result.recommendation$

[GenAI - TFIDF Feedback Loop - Promote Prompt Model]
description = Manual trigger to promote challenger prompt model to champion. Archives current champion model and registers new version in model registry. Run manually after reviewing Champion vs Challenger Report.
search = | makeresults \
| eval model_name="prompt_anomaly_model" \
| eval model_type="prompt" \
| eval model_version=strftime(now(), "%Y%m%d_%H%M%S") \
| eval algorithm="OneClassSVM" \
| eval feature_count=50 \
| eval status="champion" \
| eval created_at=now() \
| eval promoted_at=now() \
| eval promoted_by="manual_promotion" \
| eval notes="Promoted from challenger after review - includes feedback training data" \
| join type=left \
    [| inputlookup tfidf_training_data_v3 \
    | stats count AS original_samples \
    | append \
        [| inputlookup tfidf_training_feedback_lookup \
        | where has_prompt_feedback=1 AND split_assignment="train" \
        | stats count AS feedback_samples] \
    | stats sum(original_samples) AS original_samples, sum(feedback_samples) AS feedback_samples \
    | eval training_samples=original_samples+feedback_samples] \
| fillnull value=0 training_samples, feedback_samples, original_samples \
| eval _key=model_name."_".model_version \
| table _key, model_name, model_type, model_version, algorithm, feature_count, training_samples, feedback_samples, original_samples, status, created_at, promoted_at, promoted_by, notes \
| outputlookup append=true tfidf_model_registry_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
is_visible = true
alert.track = 0

[GenAI - TFIDF Feedback Loop - Promote Response Model]
description = Manual trigger to promote challenger response model to champion. Archives current champion model and registers new version in model registry. Run manually after reviewing Champion vs Challenger Report.
search = | makeresults \
| eval model_name="response_anomaly_model" \
| eval model_type="response" \
| eval model_version=strftime(now(), "%Y%m%d_%H%M%S") \
| eval algorithm="OneClassSVM" \
| eval feature_count=50 \
| eval status="champion" \
| eval created_at=now() \
| eval promoted_at=now() \
| eval promoted_by="manual_promotion" \
| eval notes="Promoted from challenger after review - includes feedback training data" \
| join type=left \
    [| inputlookup tfidf_training_data_v3 \
    | stats count AS original_samples \
    | append \
        [| inputlookup tfidf_training_feedback_lookup \
        | where has_response_feedback=1 AND split_assignment="train" \
        | stats count AS feedback_samples] \
    | stats sum(original_samples) AS original_samples, sum(feedback_samples) AS feedback_samples \
    | eval training_samples=original_samples+feedback_samples] \
| fillnull value=0 training_samples, feedback_samples, original_samples \
| eval _key=model_name."_".model_version \
| table _key, model_name, model_type, model_version, algorithm, feature_count, training_samples, feedback_samples, original_samples, status, created_at, promoted_at, promoted_by, notes \
| outputlookup append=true tfidf_model_registry_lookup
dispatch.earliest_time = -1m
dispatch.latest_time = now
enableSched = 0
is_visible = true
alert.track = 0

[GenAI - TFIDF Feedback Loop - Model Health Report]
description = Weekly summary of TF-IDF anomaly model performance, feedback statistics, and data quality metrics. Monitors feedback accumulation and model versioning status.
search = | inputlookup tfidf_training_feedback_lookup \
| stats count AS total_feedback, \
    sum(has_prompt_feedback) AS prompt_feedback_count, \
    sum(has_response_feedback) AS response_feedback_count, \
    sum(eval(if(split_assignment="train", 1, 0))) AS train_count, \
    sum(eval(if(split_assignment="valid", 1, 0))) AS valid_count, \
    sum(eval(if(split_assignment="test", 1, 0))) AS test_count, \
    dc(app_name) AS unique_apps, \
    dc(reviewer) AS unique_reviewers, \
    max(feedback_timestamp) AS last_extraction \
| append \
    [| inputlookup tfidf_model_registry_lookup \
    | where status="champion" \
    | stats count AS champion_models, \
        max(promoted_at) AS last_promotion, \
        values(model_type) AS model_types] \
| stats first(total_feedback) AS total_feedback, \
    first(prompt_feedback_count) AS prompt_feedback_count, \
    first(response_feedback_count) AS response_feedback_count, \
    first(train_count) AS train_count, \
    first(valid_count) AS valid_count, \
    first(test_count) AS test_count, \
    first(unique_apps) AS unique_apps, \
    first(unique_reviewers) AS unique_reviewers, \
    first(last_extraction) AS last_extraction, \
    last(champion_models) AS champion_models, \
    last(last_promotion) AS last_promotion \
| fillnull value=0 total_feedback, prompt_feedback_count, response_feedback_count, train_count, valid_count, test_count, champion_models \
| eval model_health=case( \
    total_feedback < 30, "WARNING - Insufficient feedback data for reliable retraining", \
    prompt_feedback_count < 10, "WARNING - Low prompt feedback volume", \
    response_feedback_count < 10, "WARNING - Low response feedback volume", \
    1=1, "HEALTHY" \
) \
| eval last_extraction_time=if(isnotnull(last_extraction), strftime(last_extraction, "%Y-%m-%d %H:%M:%S"), "No extractions yet") \
| eval last_promotion_time=if(isnotnull(last_promotion), strftime(last_promotion, "%Y-%m-%d %H:%M:%S"), "No promotions yet") \
| table total_feedback, prompt_feedback_count, response_feedback_count, \
    train_count, valid_count, test_count, \
    unique_apps, unique_reviewers, \
    champion_models, last_extraction_time, last_promotion_time, model_health
dispatch.earliest_time = -1m
dispatch.latest_time = now
cron_schedule = 15 9 * * 1
enableSched = 1
is_visible = true
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = security-team@example.com, ai-governance@example.com
action.email.subject = Weekly TF-IDF Anomaly Feedback Loop Model Health Report
action.email.message.alert = Model Health: $result.model_health$\n\nTotal Feedback: $result.total_feedback$\nPrompt Feedback: $result.prompt_feedback_count$\nResponse Feedback: $result.response_feedback_count$\n\nTrain/Valid/Test Split: $result.train_count$/$result.valid_count$/$result.test_count$\n\nChampion Models: $result.champion_models$\nLast Promotion: $result.last_promotion_time$
